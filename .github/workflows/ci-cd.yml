name: CI/CD Pipeline - AutoMax Car Dealership

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: automax-dealership
  ECS_SERVICE: automax-service
  ECS_CLUSTER: automax-cluster
  ECS_TASK_DEFINITION: automax-task-definition

jobs:
  # Job 1: Code Quality and Testing
  test:
    name: ğŸ§ª Test & Quality Checks
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: ğŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest requests

    - name: ğŸ” Lint HTML Files
      run: |
        # Install HTML linter
        npm install -g htmlhint
        htmlhint index.html

    - name: ğŸ¨ Lint CSS Files
      run: |
        # Install CSS linter
        npm install -g stylelint stylelint-config-standard
        echo '{"extends": "stylelint-config-standard"}' > .stylelintrc.json
        stylelint "*.css" || true  # Allow to continue even with warnings

    - name: âš¡ Lint JavaScript Files
      run: |
        # Install JS linter
        npm install -g eslint
        npx eslint --init --yes || true
        npx eslint "*.js" || true  # Allow to continue even with warnings

    - name: ğŸ§ª Run Website Tests
      run: |
        # Start a simple HTTP server for testing
        python -m http.server 8000 &
        sleep 5
        
        # Run Python tests
        export TEST_URL="http://localhost:8000"
        python -m pytest tests/ -v --tb=short

    - name: ğŸ“Š Generate Test Report
      if: always()
      run: |
        echo "Test execution completed"
        echo "Timestamp: $(date)"

  # Job 2: Create ECR Repository
  create-ecr:
    name: ğŸ—ï¸ Create ECR Repository
    runs-on: ubuntu-latest
    needs: test
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: ğŸ—ï¸ Create ECR Repository if not exists
      run: |
        # Check if repository exists, create if it doesn't
        aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} || \
        aws ecr create-repository \
          --repository-name ${{ env.ECR_REPOSITORY }} \
          --region ${{ env.AWS_REGION }} \
          --image-scanning-configuration scanOnPush=true \
          --encryption-configuration encryptionType=AES256
        
        echo "âœ… ECR repository ready: ${{ env.ECR_REPOSITORY }}"

  # Job 3: Build and Push Docker Image
  build:
    name: ğŸ³ Build & Push Docker Image
    runs-on: ubuntu-latest
    needs: [test, create-ecr]
    if: github.ref == 'refs/heads/main'
    
    outputs:
      image-tag: ${{ steps.image-uri.outputs.uri }}
      image-digest: ${{ steps.build.outputs.digest }}

    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: ğŸ”‘ Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2

    - name: ğŸ·ï¸ Extract Metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: ğŸ› ï¸ Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: ğŸ³ Build and Push Docker Image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: ğŸ·ï¸ Set Image URI Output
      id: image-uri
      run: |
        # Create a clean, single image URI for Terraform
        IMAGE_URI="${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest"
        echo "uri=${IMAGE_URI}" >> $GITHUB_OUTPUT
        echo "âœ… Image URI: ${IMAGE_URI}"

    - name: ğŸ” Scan Image for Vulnerabilities
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest
        format: 'sarif'
        output: 'trivy-results.sarif'
        exit-code: '0'  # Don't fail the build on vulnerabilities
        severity: 'CRITICAL,HIGH,MEDIUM'
      continue-on-error: true

    - name: ğŸ“‹ Upload Trivy Scan Results to GitHub Security
      uses: github/codeql-action/upload-sarif@v3
      if: always() && hashFiles('trivy-results.sarif') != ''
      with:
        sarif_file: 'trivy-results.sarif'
        category: 'trivy-container-scan'
      continue-on-error: true

    - name: ğŸ” Alternative Security Scan (Local Image)
      if: always()
      run: |
        echo "ğŸ”’ Running security scan on locally built image..."
        
        # Use the local image that was just built (no ECR pull needed)
        LOCAL_IMAGE="${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest"
        
        # Check if the local image exists
        if docker image inspect "$LOCAL_IMAGE" >/dev/null 2>&1; then
          echo "âœ… Scanning local image: $LOCAL_IMAGE"
          # Run Trivy scan on local image
          docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
            aquasec/trivy:latest image \
            --format table \
            --severity HIGH,CRITICAL \
            "$LOCAL_IMAGE" || true
        else
          echo "âš ï¸ Local image not found, scanning base nginx:alpine instead"
          # Fallback to scanning base image
          docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
            aquasec/trivy:latest image \
            --format table \
            --severity HIGH,CRITICAL \
            nginx:alpine || true
        fi
        
        echo "âœ… Security scan completed"

    - name: ğŸ“Š Generate Security Report Summary
      if: always()
      run: |
        echo "## ğŸ”’ Security Scan Summary" >> $GITHUB_STEP_SUMMARY
        echo "- Image: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest" >> $GITHUB_STEP_SUMMARY
        echo "- Scan Date: $(date)" >> $GITHUB_STEP_SUMMARY
        if [ -f "trivy-results.sarif" ]; then
          echo "- âœ… SARIF results generated for GitHub Security tab" >> $GITHUB_STEP_SUMMARY
        else
          echo "- âš ï¸ SARIF upload skipped (check repository permissions)" >> $GITHUB_STEP_SUMMARY
        fi
        echo "- ğŸ“Š Table format scan completed (see logs above)" >> $GITHUB_STEP_SUMMARY

  # Job 4: Deploy Infrastructure with Terraform
  deploy-infrastructure:
    name: ğŸš€ Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: [test, build]
    if: github.ref == 'refs/heads/main'
    
    defaults:
      run:
        working-directory: terraform

    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: âš™ï¸ Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.6.0

    - name: ğŸ”§ Terraform Init
      run: terraform init

    - name: ğŸ§¹ Pre-deployment State Management
      run: |
        echo "ğŸ” Comprehensive pre-deployment state check and cleanup..."
        
        # Function to safely import resource if it exists in AWS but not in state
        import_if_exists() {
          local resource_type=$1
          local resource_name=$2
          local aws_identifier=$3
          local tf_resource=$4
          
          echo "ğŸ” Checking $resource_type: $resource_name"
          
          # Check if resource exists in Terraform state
          if terraform state show "$tf_resource" &>/dev/null; then
            echo "âœ… $resource_type already in Terraform state"
            return 0
          fi
          
          # Check if resource exists in AWS
          if [ "$aws_identifier" != "" ] && [ "$aws_identifier" != "None" ] && [ "$aws_identifier" != "null" ]; then
            echo "ğŸ”„ Found $resource_type in AWS, importing to Terraform state..."
            if terraform import "$tf_resource" "$aws_identifier" 2>/dev/null; then
              echo "âœ… Successfully imported $resource_type"
              return 0
            else
              echo "âš ï¸ Failed to import $resource_type"
              return 1
            fi
          else
            echo "â„¹ï¸ $resource_type not found in AWS"
            return 0
          fi
        }
        
        # Check AWS account limits
        echo "ğŸ“Š Checking AWS account limits..."
        EIP_LIMIT=$(aws ec2 describe-account-attributes --attribute-names max-elastic-ips --query 'AccountAttributes[0].AttributeValues[0].AttributeValue' --output text)
        EIP_USED=$(aws ec2 describe-addresses --query 'length(Addresses)' --output text)
        echo "EIP Limit: $EIP_LIMIT, Used: $EIP_USED"
        
        # Clean up unused EIPs first
        if [ "$EIP_USED" -ge "$EIP_LIMIT" ]; then
          echo "ğŸ§¹ EIP limit reached, cleaning up unused EIPs..."
          aws ec2 describe-addresses --query 'Addresses[?AssociationId==null].AllocationId' --output text | \
          xargs -n1 -I {} aws ec2 release-address --allocation-id {} 2>/dev/null || echo "No unused EIPs to release"
        fi
        
        # 1. Handle ALB (multiple detection methods)
        echo "ğŸ” Checking for existing ALB..."
        LB_ARN=""
        
        # Try by exact name first
        LB_ARN=$(aws elbv2 describe-load-balancers --names automax-dealership-alb --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "")
        
        # If not found by name, search all ALBs for our pattern
        if [ "$LB_ARN" = "" ] || [ "$LB_ARN" = "None" ]; then
          LB_ARN=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `automax-dealership`)].LoadBalancerArn' --output text 2>/dev/null | head -n1 || echo "")
        fi
        
        # Import ALB if found
        if ! import_if_exists "ALB" "automax-dealership-alb" "$LB_ARN" "aws_lb.main"; then
          # If import fails and ALB exists, we need to remove it to avoid conflicts
          if [ "$LB_ARN" != "" ] && [ "$LB_ARN" != "None" ]; then
            echo "âŒ ALB import failed. This is the conflict source!"
            echo "ğŸ”„ Attempting to resolve by removing existing ALB and related resources..."
            
            # Delete listeners first
            LISTENERS=$(aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --query 'Listeners[].ListenerArn' --output text 2>/dev/null || echo "")
            if [ "$LISTENERS" != "" ]; then
              for LISTENER in $LISTENERS; do
                aws elbv2 delete-listener --listener-arn "$LISTENER" 2>/dev/null && echo "Deleted listener: $LISTENER"
              done
              sleep 10
            fi
            
            # Delete ALB
            if aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN" 2>/dev/null; then
              echo "âœ… Existing ALB deleted to resolve conflict"
              echo "â³ Waiting for deletion to complete..."
              sleep 60
              
              # Also clean up orphaned Target Groups after ALB deletion
              echo "ğŸ§¹ Cleaning up orphaned Target Groups after ALB deletion..."
              TG_ARN=$(aws elbv2 describe-target-groups --names automax-dealership-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
              if [ "$TG_ARN" != "" ] && [ "$TG_ARN" != "None" ]; then
                echo "ğŸ”„ Found orphaned Target Group, deleting: $TG_ARN"
                if aws elbv2 delete-target-group --target-group-arn "$TG_ARN" 2>/dev/null; then
                  echo "âœ… Orphaned Target Group deleted"
                else
                  echo "âš ï¸ Failed to delete Target Group (might be in use)"
                fi
                sleep 10
              fi
            else
              echo "âŒ Failed to delete ALB. Manual intervention may be required."
            fi
          fi
        fi
        
        # 2. Handle Target Group (only if ALB wasn't deleted above)
        if [ "$LB_ARN" = "" ] || [ "$LB_ARN" = "None" ]; then
          echo "ğŸ” Checking for existing Target Group (since no ALB was found)..."
          TG_ARN=$(aws elbv2 describe-target-groups --names automax-dealership-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          
          if [ "$TG_ARN" != "" ] && [ "$TG_ARN" != "None" ]; then
            echo "ğŸ”„ Found orphaned Target Group, removing before import: $TG_ARN"
            # Delete orphaned target group
            if aws elbv2 delete-target-group --target-group-arn "$TG_ARN" 2>/dev/null; then
              echo "âœ… Orphaned Target Group deleted"
            else
              echo "âš ï¸ Failed to delete Target Group, will skip import"
            fi
            sleep 10
          fi
        else
          # Try to import Target Group if ALB still exists
          TG_ARN=$(aws elbv2 describe-target-groups --names automax-dealership-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          if ! import_if_exists "Target Group" "automax-dealership-tg" "$TG_ARN" "aws_lb_target_group.main"; then
            # If import fails, delete the conflicting Target Group
            if [ "$TG_ARN" != "" ] && [ "$TG_ARN" != "None" ]; then
              echo "âŒ Target Group import failed, deleting conflicting resource: $TG_ARN"
              aws elbv2 delete-target-group --target-group-arn "$TG_ARN" 2>/dev/null && echo "âœ… Conflicting Target Group deleted"
              sleep 10
            fi
          fi
        fi
        
        # 3. Handle CloudWatch Log Group
        LOG_GROUP_NAME="/ecs/automax-dealership"
        if aws logs describe-log-groups --log-group-name-prefix "$LOG_GROUP_NAME" --query 'logGroups[0].logGroupName' --output text 2>/dev/null | grep -q "$LOG_GROUP_NAME"; then
          import_if_exists "CloudWatch Log Group" "$LOG_GROUP_NAME" "$LOG_GROUP_NAME" "aws_cloudwatch_log_group.ecs"
        fi
        
        # 4. Handle IAM Roles
        for role_name in "automax-dealership-ecs-task-execution-role" "automax-dealership-ecs-task-role"; do
          if aws iam get-role --role-name "$role_name" &>/dev/null; then
            if [ "$role_name" = "automax-dealership-ecs-task-execution-role" ]; then
              import_if_exists "IAM Role" "$role_name" "$role_name" "aws_iam_role.ecs_task_execution"
            else
              import_if_exists "IAM Role" "$role_name" "$role_name" "aws_iam_role.ecs_task"
            fi
          fi
        done
        
        echo "âœ… Pre-deployment state management completed"

    - name: ğŸ¯ ALB Conflict Resolution
      run: |
        echo "ğŸ¯ Running targeted ALB conflict resolution..."
        chmod +x resolve-alb-conflict.sh
        ./resolve-alb-conflict.sh
        echo "âœ… ALB conflict resolution completed"

    - name: ğŸ“‹ Terraform Plan
      run: |
        echo "ğŸ”„ Planning Terraform deployment..."
        
        # Create plan with detailed logging
        if ! terraform plan \
          -var="image_uri=${{ needs.build.outputs.image-tag }}" \
          -var="environment=production" \
          -detailed-exitcode \
          -out=tfplan; then
          
          PLAN_EXIT_CODE=$?
          echo "âš ï¸ Terraform plan exit code: $PLAN_EXIT_CODE"
          
          if [ $PLAN_EXIT_CODE -eq 2 ]; then
            echo "âœ… Plan succeeded with changes detected."
          else
            echo "âŒ Plan failed. Showing more details..."
            terraform plan \
              -var="image_uri=${{ needs.build.outputs.image-tag }}" \
              -var="environment=production" \
              -no-color
            exit 1
          fi
        else
          echo "âœ… Terraform plan completed successfully."
        fi

    - name: ğŸš€ Terraform Apply (with specific ALB conflict resolution)
      run: |
        echo "ğŸš€ Applying Terraform configuration..."
        
        # Capture apply output for error analysis
        if ! terraform apply -auto-approve tfplan 2>&1 | tee apply_output.log; then
          echo "âš ï¸ Initial apply failed. Analyzing error..."
          
          # Check if error is specifically about ALB already exists
          if grep -q "ELBv2 Load Balancer.*already exists" apply_output.log; then
            echo "ğŸ¯ Detected ALB 'already exists' error - this is the exact issue we're solving!"
            
            # Extract ALB name from error message
            ALB_NAME=$(grep "ELBv2 Load Balancer" apply_output.log | sed -n 's/.*ELBv2 Load Balancer (\([^)]*\)).*/\1/p' | head -n1)
            echo "ğŸ” Problematic ALB name: $ALB_NAME"
            
            # Get ALB ARN for deletion
            LB_ARN=$(aws elbv2 describe-load-balancers --names "$ALB_NAME" --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "")
            
            if [ "$LB_ARN" != "" ] && [ "$LB_ARN" != "None" ]; then
              echo "ğŸ”„ Found conflicting ALB: $LB_ARN"
              echo "ğŸ—‘ï¸ Deleting existing ALB to resolve 'already exists' conflict..."
              
              # Delete all listeners first
              LISTENERS=$(aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --query 'Listeners[].ListenerArn' --output text 2>/dev/null || echo "")
              if [ "$LISTENERS" != "" ]; then
                echo "ğŸ”„ Deleting listeners..."
                for LISTENER in $LISTENERS; do
                  aws elbv2 delete-listener --listener-arn "$LISTENER" 2>/dev/null && echo "âœ… Deleted listener: $LISTENER"
                done
                sleep 15
              fi
              
              # Now delete the ALB
              if aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN" 2>/dev/null; then
                echo "âœ… Successfully deleted conflicting ALB"
                echo "â³ Waiting for ALB deletion to propagate..."
                sleep 90  # Wait longer for full deletion
                
                # Verify deletion
                if ! aws elbv2 describe-load-balancers --names "$ALB_NAME" &>/dev/null; then
                  echo "âœ… ALB deletion confirmed"
                  
                  # Now retry the terraform apply
                  echo "ğŸ”„ Retrying Terraform apply after ALB conflict resolution..."
                  if terraform apply -auto-approve tfplan; then
                    echo "ğŸ‰ Terraform apply successful after ALB conflict resolution!"
                    exit 0
                  else
                    echo "âŒ Terraform apply still failing after ALB deletion"
                  fi
                else
                  echo "âš ï¸ ALB deletion not yet complete, continuing with full conflict resolution..."
                fi
              else
                echo "âŒ Failed to delete ALB, continuing with full conflict resolution..."
              fi
            else
              echo "âš ï¸ Could not find ALB to delete, continuing with full conflict resolution..."
            fi
          fi
          
          echo "ğŸ”„ Attempting comprehensive conflict resolution..."
          
          # Set continue on error for imports and cleanup
          set +e
          
          # First, handle ECS services that might reference old network configs
          echo "ğŸ”„ Checking for ECS service conflicts..."
          
          # Stop any existing ECS service that might have old network configuration
          if aws ecs describe-services --cluster automax-dealership-cluster --services automax-dealership-service --query 'services[0].serviceName' --output text 2>/dev/null | grep -q automax-dealership-service; then
            echo "ğŸ”„ Updating ECS service to desired count 0 to handle network changes..."
            aws ecs update-service --cluster automax-dealership-cluster --service automax-dealership-service --desired-count 0 2>/dev/null || echo "ECS service update skipped"
            sleep 30  # Wait for tasks to stop
          fi
          
          # First, try to release any unused EIPs to free up quota
          echo "ğŸ§¹ Cleaning up unused EIPs..."
          aws ec2 describe-addresses --query 'Addresses[?AssociationId==null].AllocationId' --output text | \
          xargs -n1 -I {} aws ec2 release-address --allocation-id {} 2>/dev/null || echo "No unused EIPs to release"
          
          # Handle existing load balancer and target group conflicts
          echo "ğŸ”„ Handling load balancer conflicts..."
          
          # Check if ALB exists in AWS (try multiple methods)
          echo "ğŸ” Checking for existing ALB..."
          LB_ARN=""
          
          # Method 1: Try by name
          LB_ARN=$(aws elbv2 describe-load-balancers --names automax-dealership-alb --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "")
          
          # Method 2: If name lookup fails, search by tags or list all
          if [ "$LB_ARN" = "" ] || [ "$LB_ARN" = "None" ]; then
            echo "ï¿½ ALB not found by name, searching all load balancers..."
            LB_ARN=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `automax-dealership`)].LoadBalancerArn' --output text 2>/dev/null | head -n1 || echo "")
          fi
          
          if [ "$LB_ARN" != "" ] && [ "$LB_ARN" != "None" ]; then
            echo "âœ… Found existing ALB: $LB_ARN"
            
            # Check if it's already in Terraform state
            if ! terraform state show aws_lb.main &>/dev/null; then
              echo "ğŸ”„ Importing ALB into Terraform state..."
              
              # Try importing by ARN first
              if terraform import 'aws_lb.main' "$LB_ARN"; then
                echo "âœ… ALB import by ARN successful"
              # If ARN import fails, try by name
              elif terraform import 'aws_lb.main' 'automax-dealership-alb'; then
                echo "âœ… ALB import by name successful"
              else
                echo "âŒ ALB import failed. Attempting to remove existing ALB to avoid conflicts..."
                
                # Get all listeners first
                LISTENERS=$(aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --query 'Listeners[].ListenerArn' --output text 2>/dev/null || echo "")
                
                # Delete listeners first
                if [ "$LISTENERS" != "" ]; then
                  echo "ğŸ”„ Deleting existing listeners..."
                  for LISTENER in $LISTENERS; do
                    aws elbv2 delete-listener --listener-arn "$LISTENER" 2>/dev/null || echo "Failed to delete listener $LISTENER"
                  done
                  sleep 10
                fi
                
                # Delete the load balancer
                echo "ğŸ”„ Deleting existing ALB to resolve conflict..."
                if aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN" 2>/dev/null; then
                  echo "âœ… ALB deleted successfully"
                  # Wait for ALB to be fully deleted
                  echo "â³ Waiting for ALB deletion to complete..."
                  sleep 60
                else
                  echo "âš ï¸ Failed to delete ALB. Pipeline will attempt to handle conflict."
                fi
              fi
            else
              echo "âœ… ALB already in Terraform state"
            fi
          else
            echo "â„¹ï¸ No existing ALB found"
          fi
          
          # Check if Target Group exists
          TG_ARN=$(aws elbv2 describe-target-groups --names automax-dealership-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          if [ "$TG_ARN" != "" ] && [ "$TG_ARN" != "None" ]; then
            echo "ğŸ”„ Target group exists in AWS: $TG_ARN"
            
            # Check if it's already in Terraform state
            if ! terraform state show aws_lb_target_group.main &>/dev/null; then
              echo "ğŸ”„ Importing Target Group into Terraform state..."
              if terraform import 'aws_lb_target_group.main' "$TG_ARN"; then
                echo "âœ… Target Group import successful"
              else
                echo "âš ï¸ Target Group import failed, will try to handle during apply"
              fi
            else
              echo "âœ… Target Group already in Terraform state"
            fi
            
            # Get listener ARN and import if exists
            if [ "$LB_ARN" != "" ] && [ "$LB_ARN" != "None" ]; then
              LISTENER_ARN=$(aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --query 'Listeners[0].ListenerArn' --output text 2>/dev/null || echo "")
              if [ "$LISTENER_ARN" != "" ] && [ "$LISTENER_ARN" != "None" ]; then
                echo "ğŸ”„ Listener exists in AWS: $LISTENER_ARN"
                
                # Check if it's already in Terraform state
                if ! terraform state show aws_lb_listener.main &>/dev/null; then
                  echo "ğŸ”„ Importing Listener into Terraform state..."
                  if terraform import 'aws_lb_listener.main' "$LISTENER_ARN"; then
                    echo "âœ… Listener import successful"
                  else
                    echo "âš ï¸ Listener import failed, will try to handle during apply"
                  fi
                else
                  echo "âœ… Listener already in Terraform state"
                fi
              fi
            fi
          fi
          
          # Import other resources that might exist
          echo "ğŸ”„ Importing other existing resources..."
          
          # CloudWatch Log Group
          if aws logs describe-log-groups --log-group-name-prefix "/ecs/automax-dealership" --query 'logGroups[0].logGroupName' --output text 2>/dev/null | grep -q "/ecs/automax-dealership"; then
            if ! terraform state show aws_cloudwatch_log_group.ecs &>/dev/null; then
              echo "ğŸ”„ Importing CloudWatch Log Group..."
              terraform import 'aws_cloudwatch_log_group.ecs' '/ecs/automax-dealership' || echo "Log group import failed"
            fi
          fi
          
          # IAM Roles
          if aws iam get-role --role-name automax-dealership-ecs-task-execution-role &>/dev/null; then
            if ! terraform state show aws_iam_role.ecs_task_execution &>/dev/null; then
              echo "ğŸ”„ Importing ECS Task Execution Role..."
              terraform import 'aws_iam_role.ecs_task_execution' 'automax-dealership-ecs-task-execution-role' || echo "Task execution role import failed"
            fi
          fi
          
          if aws iam get-role --role-name automax-dealership-ecs-task-role &>/dev/null; then
            if ! terraform state show aws_iam_role.ecs_task &>/dev/null; then
              echo "ğŸ”„ Importing ECS Task Role..."
              terraform import 'aws_iam_role.ecs_task' 'automax-dealership-ecs-task-role' || echo "Task role import failed"
            fi
          fi
          
          # Re-enable exit on error
          set -e
          
          echo "ğŸ”„ Re-planning after imports and cleanup..."
          terraform plan \
            -var="image_uri=${{ needs.build.outputs.image-tag }}" \
            -var="environment=production" \
            -out=tfplan-retry
          
          echo "ğŸš€ Re-applying Terraform configuration..."
          terraform apply -auto-approve tfplan-retry
        fi
        
        echo "âœ… Terraform apply completed successfully!"

    - name: ğŸ“¤ Output Infrastructure Info
      run: |
        echo "Load Balancer DNS: $(terraform output -raw load_balancer_dns)"
        echo "ECS Cluster: $(terraform output -raw ecs_cluster_name)"

  # Job 5: Deploy Application to ECS
  deploy-application:
    name: ğŸ¯ Deploy Application
    runs-on: ubuntu-latest
    needs: [build, deploy-infrastructure]
    if: github.ref == 'refs/heads/main'

    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: ğŸ“‹ Check and Download Task Definition
      run: |
        echo "ğŸ” Checking if task definition exists..."
        
        # Check if task definition exists
        if aws ecs describe-task-definition --task-definition ${{ env.ECS_TASK_DEFINITION }} &>/dev/null; then
          echo "âœ… Task definition exists, downloading current version..."
          aws ecs describe-task-definition \
            --task-definition ${{ env.ECS_TASK_DEFINITION }} \
            --query taskDefinition > task-definition.json
        else
          echo "âš ï¸ Task definition doesn't exist yet (first deployment)"
          echo "ğŸ”„ Terraform should have created it. Let's wait and retry..."
          
          # Wait a bit for Terraform resources to be fully ready
          sleep 30
          
          # Try again
          if aws ecs describe-task-definition --task-definition ${{ env.ECS_TASK_DEFINITION }} &>/dev/null; then
            echo "âœ… Task definition now exists, downloading..."
            aws ecs describe-task-definition \
              --task-definition ${{ env.ECS_TASK_DEFINITION }} \
              --query taskDefinition > task-definition.json
          else
            echo "âŒ Task definition still not found. This might be a first deployment."
            echo "ğŸ¯ Will trigger ECS service update instead of task definition update"
            echo "first-deployment=true" >> $GITHUB_OUTPUT
          fi
        fi
      id: check-task-def

    - name: ğŸ”„ Update Task Definition (if exists)
      id: task-def
      if: steps.check-task-def.outputs.first-deployment != 'true'
      uses: aws-actions/amazon-ecs-render-task-definition@v1
      with:
        task-definition: task-definition.json
        container-name: automax-container
        image: ${{ needs.build.outputs.image-tag }}

    - name: ğŸš€ Deploy to Amazon ECS (Update Existing)
      if: steps.check-task-def.outputs.first-deployment != 'true'
      uses: aws-actions/amazon-ecs-deploy-task-definition@v1
      with:
        task-definition: ${{ steps.task-def.outputs.task-definition }}
        service: ${{ env.ECS_SERVICE }}
        cluster: ${{ env.ECS_CLUSTER }}
        wait-for-service-stability: true

    - name: ğŸ¯ Deploy to Amazon ECS (First Deployment)
      if: steps.check-task-def.outputs.first-deployment == 'true'
      run: |
        echo "ğŸ¯ First deployment detected - triggering ECS service update..."
        
        # Force new deployment to pick up the Terraform-created task definition
        aws ecs update-service \
          --cluster ${{ env.ECS_CLUSTER }} \
          --service ${{ env.ECS_SERVICE }} \
          --force-new-deployment
        
        echo "âœ… ECS service deployment triggered"
        
        # Wait for deployment to complete
        echo "â³ Waiting for service to stabilize..."
        aws ecs wait services-stable \
          --cluster ${{ env.ECS_CLUSTER }} \
          --services ${{ env.ECS_SERVICE }}
        
        echo "âœ… ECS service deployment completed"

    - name: âœ… Verify Deployment
      run: |
        # Wait for deployment to complete
        sleep 30
        
        # Get load balancer URL from Terraform output
        LB_URL=$(cd terraform && terraform output -raw load_balancer_dns)
        
        # Health check
        echo "Checking application health at: http://$LB_URL/health"
        curl -f "http://$LB_URL/health" || exit 1
        
        echo "ğŸ‰ Deployment successful!"
        echo "Application is available at: http://$LB_URL"

  # Job 6: Post-Deployment Notifications
  notify:
    name: ğŸ“¢ Notify Deployment Status
    runs-on: ubuntu-latest
    needs: [deploy-application]
    if: always()

    steps:
    - name: ğŸ“¢ Slack Notification - Success
      if: ${{ needs.deploy-application.result == 'success' }}
      uses: 8398a7/action-slack@v3
      with:
        status: success
        text: 'ğŸš— AutoMax Car Dealership deployed successfully!'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}

    - name: ğŸ“¢ Slack Notification - Failure
      if: ${{ needs.deploy-application.result == 'failure' }}
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        text: 'âŒ AutoMax Car Dealership deployment failed!'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
