name: CI/CD Pipeline - AutoMax Car Dealership

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: automax-dealership
  ECS_SERVICE: automax-dealership-service
  ECS_CLUSTER: automax-dealership-cluster
  ECS_TASK_DEFINITION: automax-dealership-task-definition

jobs:
  # Job 1: Code Quality and Testing
  test:
    name: üß™ Test & Quality Checks
    runs-on: ubuntu-latest
    
    steps:
    - name: üì• Checkout Repository
      uses: actions/checkout@v4

    - name: üêç Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: üì¶ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest requests

    - name: üîç Lint HTML Files
      run: |
        # Install HTML linter
        npm install -g htmlhint
        htmlhint index.html

    - name: üé® Lint CSS Files
      run: |
        # Install CSS linter
        npm install -g stylelint stylelint-config-standard
        echo '{"extends": "stylelint-config-standard"}' > .stylelintrc.json
        stylelint "*.css" || true  # Allow to continue even with warnings

    - name: ‚ö° Lint JavaScript Files
      run: |
        # Install JS linter
        npm install -g eslint
        npx eslint --init --yes || true
        npx eslint "*.js" || true  # Allow to continue even with warnings

    - name: üß™ Run Website Tests
      run: |
        # Start a simple HTTP server for testing
        python -m http.server 8000 &
        sleep 5
        
        # Run Python tests
        export TEST_URL="http://localhost:8000"
        python -m pytest tests/ -v --tb=short

    - name: üìä Generate Test Report
      if: always()
      run: |
        echo "Test execution completed"
        echo "Timestamp: $(date)"

  # Job 2: Create ECR Repository
  create-ecr:
    name: üèóÔ∏è Create ECR Repository
    runs-on: ubuntu-latest
    needs: test
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: üì• Checkout Repository
      uses: actions/checkout@v4

    - name: üîê Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: üèóÔ∏è Create ECR Repository if not exists
      run: |
        # Check if repository exists, create if it doesn't
        aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} || \
        aws ecr create-repository \
          --repository-name ${{ env.ECR_REPOSITORY }} \
          --region ${{ env.AWS_REGION }} \
          --image-scanning-configuration scanOnPush=true \
          --encryption-configuration encryptionType=AES256
        
        echo "‚úÖ ECR repository ready: ${{ env.ECR_REPOSITORY }}"

  # Job 3: Build and Push Docker Image
  build:
    name: üê≥ Build & Push Docker Image
    runs-on: ubuntu-latest
    needs: [test, create-ecr]
    if: github.ref == 'refs/heads/main'
    
    outputs:
      image-tag: ${{ steps.image-uri.outputs.uri }}
      image-digest: ${{ steps.build.outputs.digest }}

    steps:
    - name: üì• Checkout Repository
      uses: actions/checkout@v4

    - name: üîê Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: üîë Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2

    - name: üè∑Ô∏è Extract Metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: üõ†Ô∏è Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: üê≥ Build and Push Docker Image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: üè∑Ô∏è Set Image URI Output
      id: image-uri
      run: |
        # Create a clean, single image URI for Terraform
        IMAGE_URI="${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest"
        echo "uri=${IMAGE_URI}" >> $GITHUB_OUTPUT
        echo "‚úÖ Image URI: ${IMAGE_URI}"

    - name: üîç Scan Image for Vulnerabilities
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest
        format: 'sarif'
        output: 'trivy-results.sarif'
        exit-code: '0'  # Don't fail the build on vulnerabilities
        severity: 'CRITICAL,HIGH,MEDIUM'
      continue-on-error: true

    - name: üìã Upload Trivy Scan Results to GitHub Security
      uses: github/codeql-action/upload-sarif@v3
      if: always() && hashFiles('trivy-results.sarif') != ''
      with:
        sarif_file: 'trivy-results.sarif'
        category: 'trivy-container-scan'
      continue-on-error: true

    - name: üîç Alternative Security Scan (Local Image)
      if: always()
      run: |
        echo "üîí Running security scan on locally built image..."
        
        # Use the local image that was just built (no ECR pull needed)
        LOCAL_IMAGE="${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest"
        
        # Check if the local image exists
        if docker image inspect "$LOCAL_IMAGE" >/dev/null 2>&1; then
          echo "‚úÖ Scanning local image: $LOCAL_IMAGE"
          # Run Trivy scan on local image
          docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
            aquasec/trivy:latest image \
            --format table \
            --severity HIGH,CRITICAL \
            "$LOCAL_IMAGE" || true
        else
          echo "‚ö†Ô∏è Local image not found, scanning base nginx:alpine instead"
          # Fallback to scanning base image
          docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
            aquasec/trivy:latest image \
            --format table \
            --severity HIGH,CRITICAL \
            nginx:alpine || true
        fi
        
        echo "‚úÖ Security scan completed"

    - name: üìä Generate Security Report Summary
      if: always()
      run: |
        echo "## üîí Security Scan Summary" >> $GITHUB_STEP_SUMMARY
        echo "- Image: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest" >> $GITHUB_STEP_SUMMARY
        echo "- Scan Date: $(date)" >> $GITHUB_STEP_SUMMARY
        if [ -f "trivy-results.sarif" ]; then
          echo "- ‚úÖ SARIF results generated for GitHub Security tab" >> $GITHUB_STEP_SUMMARY
        else
          echo "- ‚ö†Ô∏è SARIF upload skipped (check repository permissions)" >> $GITHUB_STEP_SUMMARY
        fi
        echo "- üìä Table format scan completed (see logs above)" >> $GITHUB_STEP_SUMMARY

  # Job 4: Deploy Infrastructure with Terraform
  deploy-infrastructure:
    name: üöÄ Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: [test, build]
    if: github.ref == 'refs/heads/main'
    
    defaults:
      run:
        working-directory: terraform

    steps:
    - name: üì• Checkout Repository
      uses: actions/checkout@v4

    - name: üîê Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: ‚öôÔ∏è Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.6.0

    - name: üîß Terraform Init
      run: terraform init

    - name: üßπ Pre-deployment State Management
      run: |
        echo "üîç Comprehensive pre-deployment state check and cleanup..."
        
        # Function to safely import resource if it exists in AWS but not in state
        import_if_exists() {
          local resource_type=$1
          local resource_name=$2
          local aws_identifier=$3
          local tf_resource=$4
          
          echo "üîç Checking $resource_type: $resource_name"
          
          # Check if resource exists in Terraform state
          if terraform state show "$tf_resource" &>/dev/null; then
            echo "‚úÖ $resource_type already in Terraform state"
            return 0
          fi
          
          # Check if resource exists in AWS
          if [ "$aws_identifier" != "" ] && [ "$aws_identifier" != "None" ] && [ "$aws_identifier" != "null" ]; then
            echo "üîÑ Found $resource_type in AWS, importing to Terraform state..."
            if terraform import "$tf_resource" "$aws_identifier" 2>/dev/null; then
              echo "‚úÖ Successfully imported $resource_type"
              return 0
            else
              echo "‚ö†Ô∏è Failed to import $resource_type"
              return 1
            fi
          else
            echo "‚ÑπÔ∏è $resource_type not found in AWS"
            return 0
          fi
        }
        
        # Check AWS account limits
        echo "üìä Checking AWS account limits..."
        EIP_LIMIT=$(aws ec2 describe-account-attributes --attribute-names max-elastic-ips --query 'AccountAttributes[0].AttributeValues[0].AttributeValue' --output text)
        EIP_USED=$(aws ec2 describe-addresses --query 'length(Addresses)' --output text)
        echo "EIP Limit: $EIP_LIMIT, Used: $EIP_USED"
        
        # Clean up unused EIPs first
        if [ "$EIP_USED" -ge "$EIP_LIMIT" ]; then
          echo "üßπ EIP limit reached, cleaning up unused EIPs..."
          aws ec2 describe-addresses --query 'Addresses[?AssociationId==null].AllocationId' --output text | \
          xargs -n1 -I {} aws ec2 release-address --allocation-id {} 2>/dev/null || echo "No unused EIPs to release"
        fi
        
        # 1. Handle ALB (multiple detection methods)
        echo "üîç Checking for existing ALB..."
        LB_ARN=""
        
        # Try by exact name first
        LB_ARN=$(aws elbv2 describe-load-balancers --names automax-dealership-alb --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "")
        
        # If not found by name, search all ALBs for our pattern
        if [ "$LB_ARN" = "" ] || [ "$LB_ARN" = "None" ]; then
          LB_ARN=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `automax-dealership`)].LoadBalancerArn' --output text 2>/dev/null | head -n1 || echo "")
        fi
        
        # Import ALB if found
        if ! import_if_exists "ALB" "automax-dealership-alb" "$LB_ARN" "aws_lb.main"; then
          # If import fails and ALB exists, we need to remove it to avoid conflicts
          if [ "$LB_ARN" != "" ] && [ "$LB_ARN" != "None" ]; then
            echo "‚ùå ALB import failed. This is the conflict source!"
            echo "üîÑ Attempting to resolve by removing existing ALB and related resources..."
            
            # Delete listeners first
            LISTENERS=$(aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --query 'Listeners[].ListenerArn' --output text 2>/dev/null || echo "")
            if [ "$LISTENERS" != "" ]; then
              for LISTENER in $LISTENERS; do
                aws elbv2 delete-listener --listener-arn "$LISTENER" 2>/dev/null && echo "Deleted listener: $LISTENER"
              done
              sleep 10
            fi
            
            # Delete ALB
            if aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN" 2>/dev/null; then
              echo "‚úÖ Existing ALB deleted to resolve conflict"
              echo "‚è≥ Waiting for deletion to complete..."
              sleep 60
              
              # Also clean up orphaned Target Groups after ALB deletion
              echo "üßπ Cleaning up orphaned Target Groups after ALB deletion..."
              TG_ARN=$(aws elbv2 describe-target-groups --names automax-dealership-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
              if [ "$TG_ARN" != "" ] && [ "$TG_ARN" != "None" ]; then
                echo "üîÑ Found orphaned Target Group, deleting: $TG_ARN"
                if aws elbv2 delete-target-group --target-group-arn "$TG_ARN" 2>/dev/null; then
                  echo "‚úÖ Orphaned Target Group deleted"
                else
                  echo "‚ö†Ô∏è Failed to delete Target Group (might be in use)"
                fi
                sleep 10
              fi
            else
              echo "‚ùå Failed to delete ALB. Manual intervention may be required."
            fi
          fi
        fi
        
        # 2. Handle Target Group (only if ALB wasn't deleted above)
        if [ "$LB_ARN" = "" ] || [ "$LB_ARN" = "None" ]; then
          echo "üîç Checking for existing Target Group (since no ALB was found)..."
          TG_ARN=$(aws elbv2 describe-target-groups --names automax-dealership-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          
          if [ "$TG_ARN" != "" ] && [ "$TG_ARN" != "None" ]; then
            echo "üîÑ Found orphaned Target Group, removing before import: $TG_ARN"
            # Delete orphaned target group
            if aws elbv2 delete-target-group --target-group-arn "$TG_ARN" 2>/dev/null; then
              echo "‚úÖ Orphaned Target Group deleted"
            else
              echo "‚ö†Ô∏è Failed to delete Target Group, will skip import"
            fi
            sleep 10
          fi
        else
          # Try to import Target Group if ALB still exists
          TG_ARN=$(aws elbv2 describe-target-groups --names automax-dealership-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          if ! import_if_exists "Target Group" "automax-dealership-tg" "$TG_ARN" "aws_lb_target_group.main"; then
            # If import fails, delete the conflicting Target Group
            if [ "$TG_ARN" != "" ] && [ "$TG_ARN" != "None" ]; then
              echo "‚ùå Target Group import failed, deleting conflicting resource: $TG_ARN"
              aws elbv2 delete-target-group --target-group-arn "$TG_ARN" 2>/dev/null && echo "‚úÖ Conflicting Target Group deleted"
              sleep 10
            fi
          fi
        fi
        
        # 3. Handle CloudWatch Log Group
        LOG_GROUP_NAME="/ecs/automax-dealership"
        echo "üîç Checking for existing CloudWatch Log Group..."
        if aws logs describe-log-groups --log-group-name-prefix "$LOG_GROUP_NAME" --query 'logGroups[0].logGroupName' --output text 2>/dev/null | grep -q "$LOG_GROUP_NAME"; then
          if ! import_if_exists "CloudWatch Log Group" "$LOG_GROUP_NAME" "$LOG_GROUP_NAME" "aws_cloudwatch_log_group.ecs"; then
            # If import fails, delete the existing log group
            echo "‚ùå CloudWatch Log Group import failed, deleting existing log group..."
            if aws logs delete-log-group --log-group-name "$LOG_GROUP_NAME" 2>/dev/null; then
              echo "‚úÖ Existing CloudWatch Log Group deleted"
              sleep 5
            else
              echo "‚ö†Ô∏è Failed to delete CloudWatch Log Group"
            fi
          fi
        else
          echo "‚ÑπÔ∏è No existing CloudWatch Log Group found"
        fi
        
        # 4. Handle IAM Roles
        echo "üîç Checking for existing IAM Roles..."
        for role_name in "automax-dealership-ecs-task-execution-role" "automax-dealership-ecs-task-role"; do
          if aws iam get-role --role-name "$role_name" &>/dev/null; then
            echo "üîÑ Found existing IAM Role: $role_name"
            
            if [ "$role_name" = "automax-dealership-ecs-task-execution-role" ]; then
              if ! import_if_exists "IAM Role" "$role_name" "$role_name" "aws_iam_role.ecs_task_execution"; then
                echo "‚ùå IAM Role import failed for $role_name"
                echo "‚ö†Ô∏è Skipping IAM role deletion (requires careful handling)"
              fi
            else
              if ! import_if_exists "IAM Role" "$role_name" "$role_name" "aws_iam_role.ecs_task"; then
                echo "‚ùå IAM Role import failed for $role_name"
                echo "‚ö†Ô∏è Skipping IAM role deletion (requires careful handling)"
              fi
            fi
          else
            echo "‚ÑπÔ∏è IAM Role not found: $role_name"
          fi
        done
        
        echo "‚úÖ Pre-deployment state management completed"

    - name: üéØ ALB Conflict Resolution
      run: |
        echo "üéØ Running targeted ALB conflict resolution..."
        chmod +x resolve-alb-conflict.sh
        ./resolve-alb-conflict.sh
        echo "‚úÖ ALB conflict resolution completed"

    - name: üìã Terraform Plan
      run: |
        echo "üîÑ Planning Terraform deployment..."
        
        # Create plan with detailed logging
        if ! terraform plan \
          -var="image_uri=${{ needs.build.outputs.image-tag }}" \
          -var="environment=production" \
          -detailed-exitcode \
          -out=tfplan; then
          
          PLAN_EXIT_CODE=$?
          echo "‚ö†Ô∏è Terraform plan exit code: $PLAN_EXIT_CODE"
          
          if [ $PLAN_EXIT_CODE -eq 2 ]; then
            echo "‚úÖ Plan succeeded with changes detected."
          else
            echo "‚ùå Plan failed. Showing more details..."
            terraform plan \
              -var="image_uri=${{ needs.build.outputs.image-tag }}" \
              -var="environment=production" \
              -no-color
            exit 1
          fi
        else
          echo "‚úÖ Terraform plan completed successfully."
        fi

    - name: üöÄ Terraform Apply (with specific ALB conflict resolution)
      run: |
        echo "üöÄ Applying Terraform configuration..."
        
        # Capture apply output for error analysis
        if ! terraform apply -auto-approve tfplan 2>&1 | tee apply_output.log; then
          echo "‚ö†Ô∏è Initial apply failed. Analyzing error..."
          
          # Check if error is specifically about ALB already exists
          if grep -q "ELBv2 Load Balancer.*already exists" apply_output.log; then
            echo "üéØ Detected ALB 'already exists' error - this is the exact issue we're solving!"
            
            # Extract ALB name from error message
            ALB_NAME=$(grep "ELBv2 Load Balancer" apply_output.log | sed -n 's/.*ELBv2 Load Balancer (\([^)]*\)).*/\1/p' | head -n1)
            echo "üîç Problematic ALB name: $ALB_NAME"
            
            # Get ALB ARN for deletion
            LB_ARN=$(aws elbv2 describe-load-balancers --names "$ALB_NAME" --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "")
            
            if [ "$LB_ARN" != "" ] && [ "$LB_ARN" != "None" ]; then
              echo "üîÑ Found conflicting ALB: $LB_ARN"
              echo "üóëÔ∏è Deleting existing ALB to resolve 'already exists' conflict..."
              
              # Delete all listeners first
              LISTENERS=$(aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --query 'Listeners[].ListenerArn' --output text 2>/dev/null || echo "")
              if [ "$LISTENERS" != "" ]; then
                echo "üîÑ Deleting listeners..."
                for LISTENER in $LISTENERS; do
                  aws elbv2 delete-listener --listener-arn "$LISTENER" 2>/dev/null && echo "‚úÖ Deleted listener: $LISTENER"
                done
                sleep 15
              fi
              
              # Now delete the ALB
              if aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN" 2>/dev/null; then
                echo "‚úÖ Successfully deleted conflicting ALB"
                echo "‚è≥ Waiting for ALB deletion to propagate..."
                sleep 90  # Wait longer for full deletion
                
                # Verify deletion
                if ! aws elbv2 describe-load-balancers --names "$ALB_NAME" &>/dev/null; then
                  echo "‚úÖ ALB deletion confirmed"
                  
                  # Now retry the terraform apply
                  echo "üîÑ Retrying Terraform apply after ALB conflict resolution..."
                  if terraform apply -auto-approve tfplan; then
                    echo "üéâ Terraform apply successful after ALB conflict resolution!"
                    exit 0
                  else
                    echo "‚ùå Terraform apply still failing after ALB deletion"
                  fi
                else
                  echo "‚ö†Ô∏è ALB deletion not yet complete, continuing with full conflict resolution..."
                fi
              else
                echo "‚ùå Failed to delete ALB, continuing with full conflict resolution..."
              fi
            else
              echo "‚ö†Ô∏è Could not find ALB to delete, continuing with full conflict resolution..."
            fi
          fi
          
          echo "üîÑ Attempting comprehensive conflict resolution..."
          
          # Set continue on error for imports and cleanup
          set +e
          
          # First, handle ECS services that might reference old network configs
          echo "üîÑ Checking for ECS service conflicts..."
          
          # Stop any existing ECS service that might have old network configuration
          if aws ecs describe-services --cluster automax-dealership-cluster --services automax-dealership-service --query 'services[0].serviceName' --output text 2>/dev/null | grep -q automax-dealership-service; then
            echo "üîÑ Updating ECS service to desired count 0 to handle network changes..."
            aws ecs update-service --cluster automax-dealership-cluster --service automax-dealership-service --desired-count 0 2>/dev/null || echo "ECS service update skipped"
            sleep 30  # Wait for tasks to stop
          fi
          
          # First, try to release any unused EIPs to free up quota
          echo "üßπ Cleaning up unused EIPs..."
          aws ec2 describe-addresses --query 'Addresses[?AssociationId==null].AllocationId' --output text | \
          xargs -n1 -I {} aws ec2 release-address --allocation-id {} 2>/dev/null || echo "No unused EIPs to release"
          
          # Handle existing load balancer and target group conflicts
          echo "üîÑ Handling load balancer conflicts..."
          
          # Check if ALB exists in AWS (try multiple methods)
          echo "üîç Checking for existing ALB..."
          LB_ARN=""
          
          # Method 1: Try by name
          LB_ARN=$(aws elbv2 describe-load-balancers --names automax-dealership-alb --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "")
          
          # Method 2: If name lookup fails, search by tags or list all
          if [ "$LB_ARN" = "" ] || [ "$LB_ARN" = "None" ]; then
            echo "ÔøΩ ALB not found by name, searching all load balancers..."
            LB_ARN=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `automax-dealership`)].LoadBalancerArn' --output text 2>/dev/null | head -n1 || echo "")
          fi
          
          if [ "$LB_ARN" != "" ] && [ "$LB_ARN" != "None" ]; then
            echo "‚úÖ Found existing ALB: $LB_ARN"
            
            # Check if it's already in Terraform state
            if ! terraform state show aws_lb.main &>/dev/null; then
              echo "üîÑ Importing ALB into Terraform state..."
              
              # Try importing by ARN first
              if terraform import 'aws_lb.main' "$LB_ARN"; then
                echo "‚úÖ ALB import by ARN successful"
              # If ARN import fails, try by name
              elif terraform import 'aws_lb.main' 'automax-dealership-alb'; then
                echo "‚úÖ ALB import by name successful"
              else
                echo "‚ùå ALB import failed. Attempting to remove existing ALB to avoid conflicts..."
                
                # Get all listeners first
                LISTENERS=$(aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --query 'Listeners[].ListenerArn' --output text 2>/dev/null || echo "")
                
                # Delete listeners first
                if [ "$LISTENERS" != "" ]; then
                  echo "üîÑ Deleting existing listeners..."
                  for LISTENER in $LISTENERS; do
                    aws elbv2 delete-listener --listener-arn "$LISTENER" 2>/dev/null || echo "Failed to delete listener $LISTENER"
                  done
                  sleep 10
                fi
                
                # Delete the load balancer
                echo "üîÑ Deleting existing ALB to resolve conflict..."
                if aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN" 2>/dev/null; then
                  echo "‚úÖ ALB deleted successfully"
                  # Wait for ALB to be fully deleted
                  echo "‚è≥ Waiting for ALB deletion to complete..."
                  sleep 60
                else
                  echo "‚ö†Ô∏è Failed to delete ALB. Pipeline will attempt to handle conflict."
                fi
              fi
            else
              echo "‚úÖ ALB already in Terraform state"
            fi
          else
            echo "‚ÑπÔ∏è No existing ALB found"
          fi
          
          # Check if Target Group exists
          TG_ARN=$(aws elbv2 describe-target-groups --names automax-dealership-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          if [ "$TG_ARN" != "" ] && [ "$TG_ARN" != "None" ]; then
            echo "üîÑ Target group exists in AWS: $TG_ARN"
            
            # Check if it's already in Terraform state
            if ! terraform state show aws_lb_target_group.main &>/dev/null; then
              echo "üîÑ Importing Target Group into Terraform state..."
              if terraform import 'aws_lb_target_group.main' "$TG_ARN"; then
                echo "‚úÖ Target Group import successful"
              else
                echo "‚ö†Ô∏è Target Group import failed, will try to handle during apply"
              fi
            else
              echo "‚úÖ Target Group already in Terraform state"
            fi
            
            # Get listener ARN and import if exists
            if [ "$LB_ARN" != "" ] && [ "$LB_ARN" != "None" ]; then
              LISTENER_ARN=$(aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --query 'Listeners[0].ListenerArn' --output text 2>/dev/null || echo "")
              if [ "$LISTENER_ARN" != "" ] && [ "$LISTENER_ARN" != "None" ]; then
                echo "üîÑ Listener exists in AWS: $LISTENER_ARN"
                
                # Check if it's already in Terraform state
                if ! terraform state show aws_lb_listener.main &>/dev/null; then
                  echo "üîÑ Importing Listener into Terraform state..."
                  if terraform import 'aws_lb_listener.main' "$LISTENER_ARN"; then
                    echo "‚úÖ Listener import successful"
                  else
                    echo "‚ö†Ô∏è Listener import failed, will try to handle during apply"
                  fi
                else
                  echo "‚úÖ Listener already in Terraform state"
                fi
              fi
            fi
          fi
          
          # Import other resources that might exist
          echo "üîÑ Importing other existing resources..."
          
          # CloudWatch Log Group
          if aws logs describe-log-groups --log-group-name-prefix "/ecs/automax-dealership" --query 'logGroups[0].logGroupName' --output text 2>/dev/null | grep -q "/ecs/automax-dealership"; then
            if ! terraform state show aws_cloudwatch_log_group.ecs &>/dev/null; then
              echo "üîÑ Importing CloudWatch Log Group..."
              if terraform import 'aws_cloudwatch_log_group.ecs' '/ecs/automax-dealership' 2>/dev/null; then
                echo "‚úÖ CloudWatch Log Group import successful"
              else
                echo "‚ùå CloudWatch Log Group import failed, deleting existing log group..."
                aws logs delete-log-group --log-group-name "/ecs/automax-dealership" 2>/dev/null || echo "Failed to delete log group"
                sleep 5
              fi
            fi
          fi
          
          # IAM Roles
          if aws iam get-role --role-name automax-dealership-ecs-task-execution-role &>/dev/null; then
            if ! terraform state show aws_iam_role.ecs_task_execution &>/dev/null; then
              echo "üîÑ Importing ECS Task Execution Role..."
              terraform import 'aws_iam_role.ecs_task_execution' 'automax-dealership-ecs-task-execution-role' || echo "Task execution role import failed"
            fi
          fi
          
          if aws iam get-role --role-name automax-dealership-ecs-task-role &>/dev/null; then
            if ! terraform state show aws_iam_role.ecs_task &>/dev/null; then
              echo "üîÑ Importing ECS Task Role..."
              terraform import 'aws_iam_role.ecs_task' 'automax-dealership-ecs-task-role' || echo "Task role import failed"
            fi
          fi
          
          # Re-enable exit on error
          set -e
          
          echo "üîÑ Re-planning after imports and cleanup..."
          terraform plan \
            -var="image_uri=${{ needs.build.outputs.image-tag }}" \
            -var="environment=production" \
            -out=tfplan-retry
          
          echo "üöÄ Re-applying Terraform configuration..."
          terraform apply -auto-approve tfplan-retry
        fi
        
        echo "‚úÖ Terraform apply completed successfully!"

    - name: üì§ Output Infrastructure Info
      run: |
        echo "Load Balancer DNS: $(terraform output -raw load_balancer_dns)"
        echo "ECS Cluster: $(terraform output -raw ecs_cluster_name)"

    - name: ‚úÖ Verify Infrastructure Readiness
      run: |
        echo "üîç Verifying that all infrastructure components are ready..."
        
        # Verify ECS Cluster
        CLUSTER_NAME=$(terraform output -raw ecs_cluster_name)
        echo "üîç Verifying ECS cluster: $CLUSTER_NAME"
        if aws ecs describe-clusters --clusters "$CLUSTER_NAME" --query 'clusters[0].status' --output text | grep -q "ACTIVE"; then
          echo "‚úÖ ECS cluster is active"
        else
          echo "‚ùå ECS cluster is not active"
          exit 1
        fi
        
        # Verify ECS Service
        SERVICE_NAME=$(terraform output -raw ecs_service_name)
        echo "üîç Verifying ECS service: $SERVICE_NAME"
        if aws ecs describe-services --cluster "$CLUSTER_NAME" --services "$SERVICE_NAME" --query 'services[0].serviceName' --output text 2>/dev/null | grep -q "$SERVICE_NAME"; then
          echo "‚úÖ ECS service exists"
        else
          echo "‚ùå ECS service not found"
          exit 1
        fi
        
        # Verify ALB
        ALB_DNS=$(terraform output -raw load_balancer_dns)
        echo "üîç Verifying ALB: $ALB_DNS"
        if [ "$ALB_DNS" != "" ] && [ "$ALB_DNS" != "null" ]; then
          echo "‚úÖ ALB is ready"
        else
          echo "‚ùå ALB DNS not available"
          exit 1
        fi
        
        echo "üéâ All infrastructure components are ready for application deployment!"

  # Job 5: Deploy Application to ECS
  deploy-application:
    name: üéØ Deploy Application
    runs-on: ubuntu-latest
    needs: [build, deploy-infrastructure]
    if: github.ref == 'refs/heads/main'

    steps:
    - name: üì• Checkout Repository
      uses: actions/checkout@v4

    - name: üîê Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: üìã Check and Download Task Definition
      run: |
        echo "üîç Checking if task definition exists..."
        
        # Get the actual task definition name from the environment (now corrected)
        TASK_DEF_NAME="${{ env.ECS_TASK_DEFINITION }}"
        echo "üîç Looking for task definition: $TASK_DEF_NAME"
        
        # Check if task definition exists
        if aws ecs describe-task-definition --task-definition "$TASK_DEF_NAME" &>/dev/null; then
          echo "‚úÖ Task definition exists, downloading current version..."
          aws ecs describe-task-definition \
            --task-definition "$TASK_DEF_NAME" \
            --query taskDefinition > task-definition.json
        else
          echo "‚ö†Ô∏è Task definition doesn't exist yet (first deployment)"
          echo "üîÑ Terraform should have created it. Let's wait and retry..."
          
          # Wait a bit for Terraform resources to be fully ready
          sleep 30
          
          # Try again
          if aws ecs describe-task-definition --task-definition "$TASK_DEF_NAME" &>/dev/null; then
            echo "‚úÖ Task definition now exists, downloading..."
            aws ecs describe-task-definition \
              --task-definition "$TASK_DEF_NAME" \
              --query taskDefinition > task-definition.json
          else
            echo "‚ùå Task definition still not found. This might be a first deployment."
            echo "üéØ Will trigger ECS service update instead of task definition update"
            echo "first-deployment=true" >> $GITHUB_OUTPUT
          fi
        fi
      id: check-task-def

    - name: üîÑ Update Task Definition (if exists)
      id: task-def
      if: steps.check-task-def.outputs.first-deployment != 'true'
      uses: aws-actions/amazon-ecs-render-task-definition@v1
      with:
        task-definition: task-definition.json
        container-name: automax-container
        image: ${{ needs.build.outputs.image-tag }}

    - name: üöÄ Deploy to Amazon ECS (Update Existing)
      if: steps.check-task-def.outputs.first-deployment != 'true'
      uses: aws-actions/amazon-ecs-deploy-task-definition@v1
      with:
        task-definition: ${{ steps.task-def.outputs.task-definition }}
        service: ${{ env.ECS_SERVICE }}
        cluster: ${{ env.ECS_CLUSTER }}
        wait-for-service-stability: true

    - name: üéØ Deploy to Amazon ECS (First Deployment)
      if: steps.check-task-def.outputs.first-deployment == 'true'
      run: |
        echo "üéØ First deployment detected - verifying ECS infrastructure..."
        
        # Get the actual resource names from Terraform outputs
        cd terraform
        CLUSTER_NAME=$(terraform output -raw ecs_cluster_name)
        SERVICE_NAME=$(terraform output -raw ecs_service_name)
        cd ..
        
        echo "üîç Using Terraform-created resources:"
        echo "  Cluster: $CLUSTER_NAME"
        echo "  Service: $SERVICE_NAME"
        
        # First, verify that the ECS cluster exists
        echo "üîç Checking if ECS cluster exists..."
        if ! aws ecs describe-clusters --clusters "$CLUSTER_NAME" --query 'clusters[0].clusterName' --output text 2>/dev/null | grep -q "$CLUSTER_NAME"; then
          echo "‚ùå ECS cluster not found: $CLUSTER_NAME"
          echo "üîÑ This might be a Terraform timing issue. Waiting for cluster creation..."
          sleep 30
          
          # Check again
          if ! aws ecs describe-clusters --clusters "$CLUSTER_NAME" --query 'clusters[0].clusterName' --output text 2>/dev/null | grep -q "$CLUSTER_NAME"; then
            echo "‚ùå ECS cluster still not found after waiting. Terraform may have failed to create it."
            echo "üîç Listing all available clusters:"
            aws ecs list-clusters --query 'clusterArns' --output table || echo "No clusters found"
            exit 1
          fi
        fi
        echo "‚úÖ ECS cluster found: $CLUSTER_NAME"
        
        # Next, verify that the ECS service exists
        echo "üîç Checking if ECS service exists..."
        if ! aws ecs describe-services --cluster "$CLUSTER_NAME" --services "$SERVICE_NAME" --query 'services[0].serviceName' --output text 2>/dev/null | grep -q "$SERVICE_NAME"; then
          echo "‚ùå ECS service not found: $SERVICE_NAME"
          echo "üîÑ This might be a Terraform timing issue. Waiting for service creation..."
          sleep 30
          
          # Check again
          if ! aws ecs describe-services --cluster "$CLUSTER_NAME" --services "$SERVICE_NAME" --query 'services[0].serviceName' --output text 2>/dev/null | grep -q "$SERVICE_NAME"; then
            echo "‚ùå ECS service still not found after waiting. Terraform may have failed to create it."
            echo "üîç Listing all services in cluster:"
            aws ecs list-services --cluster "$CLUSTER_NAME" --query 'serviceArns' --output table || echo "No services found"
            exit 1
          fi
        fi
        echo "‚úÖ ECS service found: $SERVICE_NAME"
        
        # Now safely trigger the service update
        echo "üöÄ Triggering ECS service update..."
        aws ecs update-service \
          --cluster "$CLUSTER_NAME" \
          --service "$SERVICE_NAME" \
          --force-new-deployment
        
        echo "‚úÖ ECS service deployment triggered"
        
        # Wait for deployment to complete
        echo "‚è≥ Waiting for service to stabilize..."
        aws ecs wait services-stable \
          --cluster "$CLUSTER_NAME" \
          --services "$SERVICE_NAME"
        
        echo "‚úÖ ECS service deployment completed"

    - name: ‚úÖ Verify Deployment
      run: |
        # Wait for deployment to complete
        sleep 30
        
        # Get load balancer URL from Terraform output
        LB_URL=$(cd terraform && terraform output -raw load_balancer_dns)
        
        # Health check
        echo "Checking application health at: http://$LB_URL/health"
        curl -f "http://$LB_URL/health" || exit 1
        
        echo "üéâ Deployment successful!"
        echo "Application is available at: http://$LB_URL"

  # Job 6: Post-Deployment Notifications
  notify:
    name: üì¢ Notify Deployment Status
    runs-on: ubuntu-latest
    needs: [deploy-application]
    if: always()

    steps:
    - name: üì¢ Slack Notification - Success
      if: ${{ needs.deploy-application.result == 'success' }}
      uses: 8398a7/action-slack@v3
      with:
        status: success
        text: 'üöó AutoMax Car Dealership deployed successfully!'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}

    - name: üì¢ Slack Notification - Failure
      if: ${{ needs.deploy-application.result == 'failure' }}
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        text: '‚ùå AutoMax Car Dealership deployment failed!'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
