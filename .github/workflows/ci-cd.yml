name: CI/CD Pipeline - AutoMax Car Dealership

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: automax-dealership
  ECS_SERVICE: automax-dealership-service
  ECS_CLUSTER: automax-dealership-cluster
  ECS_TASK_DEFINITION: automax-dealership-task-definition

jobs:
  # Job 1: Code Quality and Testing
  test:
    name: ğŸ§ª Test & Quality Checks
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: ğŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest requests

    - name: ğŸ” Lint HTML Files
      run: |
        # Install HTML linter
        npm install -g htmlhint
        htmlhint index.html

    - name: ğŸ¨ Lint CSS Files
      run: |
        # Install CSS linter
        npm install -g stylelint stylelint-config-standard
        echo '{"extends": "stylelint-config-standard"}' > .stylelintrc.json
        stylelint "*.css" || true  # Allow to continue even with warnings

    - name: âš¡ Lint JavaScript Files
      run: |
        # Install JS linter
        npm install -g eslint
        npx eslint --init --yes || true
        npx eslint "*.js" || true  # Allow to continue even with warnings

    - name: ğŸ§ª Run Website Tests
      run: |
        # Start a simple HTTP server for testing
        python -m http.server 8000 &
        sleep 5
        
        # Run Python tests
        export TEST_URL="http://localhost:8000"
        python -m pytest tests/ -v --tb=short

    - name: ğŸ“Š Generate Test Report
      if: always()
      run: |
        echo "Test execution completed"
        echo "Timestamp: $(date)"

  # Job 2: Create ECR Repository
  create-ecr:
    name: ğŸ—ï¸ Create ECR Repository
    runs-on: ubuntu-latest
    needs: test
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: ğŸ—ï¸ Create ECR Repository if not exists
      run: |
        # Check if repository exists, create if it doesn't
        aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} || \
        aws ecr create-repository \
          --repository-name ${{ env.ECR_REPOSITORY }} \
          --region ${{ env.AWS_REGION }} \
          --image-scanning-configuration scanOnPush=true \
          --encryption-configuration encryptionType=AES256
        
        echo "âœ… ECR repository ready: ${{ env.ECR_REPOSITORY }}"

  # Job 3: Build and Push Docker Image
  build:
    name: ğŸ³ Build & Push Docker Image
    runs-on: ubuntu-latest
    needs: [test, create-ecr]
    if: github.ref == 'refs/heads/main'
    
    outputs:
      image-tag: ${{ steps.image-uri.outputs.uri }}
      image-digest: ${{ steps.build.outputs.digest }}

    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: ğŸ”‘ Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2

    - name: ğŸ·ï¸ Extract Metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: ğŸ› ï¸ Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: ğŸ³ Build and Push Docker Image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: ğŸ·ï¸ Set Image URI Output
      id: image-uri
      run: |
        # Create a clean, single image URI for Terraform
        IMAGE_URI="${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest"
        echo "uri=${IMAGE_URI}" >> $GITHUB_OUTPUT
        echo "âœ… Image URI: ${IMAGE_URI}"

    - name: ğŸ” Scan Image for Vulnerabilities
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest
        format: 'sarif'
        output: 'trivy-results.sarif'
        exit-code: '0'  # Don't fail the build on vulnerabilities
        severity: 'CRITICAL,HIGH,MEDIUM'
      continue-on-error: true

    - name: ğŸ“‹ Upload Trivy Scan Results to GitHub Security
      uses: github/codeql-action/upload-sarif@v3
      if: always() && hashFiles('trivy-results.sarif') != ''
      with:
        sarif_file: 'trivy-results.sarif'
        category: 'trivy-container-scan'
      continue-on-error: true

    - name: ğŸ” Alternative Security Scan (Local Image)
      if: always()
      run: |
        echo "ğŸ”’ Running security scan on locally built image..."
        
        # Use the local image that was just built (no ECR pull needed)
        LOCAL_IMAGE="${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest"
        
        # Check if the local image exists
        if docker image inspect "$LOCAL_IMAGE" >/dev/null 2>&1; then
          echo "âœ… Scanning local image: $LOCAL_IMAGE"
          # Run Trivy scan on local image
          docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
            aquasec/trivy:latest image \
            --format table \
            --severity HIGH,CRITICAL \
            "$LOCAL_IMAGE" || true
        else
          echo "âš ï¸ Local image not found, scanning base nginx:alpine instead"
          # Fallback to scanning base image
          docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
            aquasec/trivy:latest image \
            --format table \
            --severity HIGH,CRITICAL \
            nginx:alpine || true
        fi
        
        echo "âœ… Security scan completed"

    - name: ğŸ“Š Generate Security Report Summary
      if: always()
      run: |
        echo "## ğŸ”’ Security Scan Summary" >> $GITHUB_STEP_SUMMARY
        echo "- Image: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest" >> $GITHUB_STEP_SUMMARY
        echo "- Scan Date: $(date)" >> $GITHUB_STEP_SUMMARY
        if [ -f "trivy-results.sarif" ]; then
          echo "- âœ… SARIF results generated for GitHub Security tab" >> $GITHUB_STEP_SUMMARY
        else
          echo "- âš ï¸ SARIF upload skipped (check repository permissions)" >> $GITHUB_STEP_SUMMARY
        fi
        echo "- ğŸ“Š Table format scan completed (see logs above)" >> $GITHUB_STEP_SUMMARY

  # Job 4: Deploy Infrastructure with Terraform
  deploy-infrastructure:
    name: ğŸš€ Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: [test, build]
    if: github.ref == 'refs/heads/main'
    
    defaults:
      run:
        working-directory: terraform

    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: âš™ï¸ Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.6.0

    - name: ğŸ”§ Terraform Init
      run: terraform init

    - name: ğŸ§¹ Pre-deployment State Management
      run: |
        echo "ğŸ” Comprehensive pre-deployment state check and cleanup..."
        
        # Function to safely import resource if it exists in AWS but not in state
        import_if_exists() {
          local resource_type=$1
          local resource_name=$2
          local aws_identifier=$3
          local tf_resource=$4
          
          echo "ğŸ” Checking $resource_type: $resource_name"
          
          # Check if resource exists in Terraform state
          if terraform state show "$tf_resource" &>/dev/null; then
            echo "âœ… $resource_type already in Terraform state"
            return 0
          fi
          
          # Check if resource exists in AWS
          if [ "$aws_identifier" != "" ] && [ "$aws_identifier" != "None" ] && [ "$aws_identifier" != "null" ]; then
            echo "ğŸ”„ Found $resource_type in AWS, importing to Terraform state..."
            if terraform import "$tf_resource" "$aws_identifier" 2>/dev/null; then
              echo "âœ… Successfully imported $resource_type"
              return 0
            else
              echo "âš ï¸ Failed to import $resource_type"
              return 1
            fi
          else
            echo "â„¹ï¸ $resource_type not found in AWS"
            return 0
          fi
        }
        
        # Check AWS account limits
        echo "ğŸ“Š Checking AWS account limits..."
        EIP_LIMIT=$(aws ec2 describe-account-attributes --attribute-names max-elastic-ips --query 'AccountAttributes[0].AttributeValues[0].AttributeValue' --output text)
        EIP_USED=$(aws ec2 describe-addresses --query 'length(Addresses)' --output text)
        echo "EIP Limit: $EIP_LIMIT, Used: $EIP_USED"
        
        # Clean up unused EIPs first
        if [ "$EIP_USED" -ge "$EIP_LIMIT" ]; then
          echo "ğŸ§¹ EIP limit reached, cleaning up unused EIPs..."
          aws ec2 describe-addresses --query 'Addresses[?AssociationId==null].AllocationId' --output text | \
          xargs -n1 -I {} aws ec2 release-address --allocation-id {} 2>/dev/null || echo "No unused EIPs to release"
        fi
        
        # 1. Handle ALB (multiple detection methods)
        echo "ğŸ” Checking for existing ALB..."
        LB_ARN=""
        
        # Try by exact name first
        LB_ARN=$(aws elbv2 describe-load-balancers --names automax-dealership-alb --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "")
        
        # If not found by name, search all ALBs for our pattern
        if [ "$LB_ARN" = "" ] || [ "$LB_ARN" = "None" ]; then
          LB_ARN=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `automax-dealership`)].LoadBalancerArn' --output text 2>/dev/null | head -n1 || echo "")
        fi
        
        # Import ALB if found
        if ! import_if_exists "ALB" "automax-dealership-alb" "$LB_ARN" "aws_lb.main"; then
          # If import fails and ALB exists, we need to remove it to avoid conflicts
          if [ "$LB_ARN" != "" ] && [ "$LB_ARN" != "None" ]; then
            echo "âŒ ALB import failed. This is the conflict source!"
            echo "ğŸ”„ Attempting to resolve by removing existing ALB and related resources..."
            
            # Delete listeners first
            LISTENERS=$(aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --query 'Listeners[].ListenerArn' --output text 2>/dev/null || echo "")
            if [ "$LISTENERS" != "" ]; then
              for LISTENER in $LISTENERS; do
                aws elbv2 delete-listener --listener-arn "$LISTENER" 2>/dev/null && echo "Deleted listener: $LISTENER"
              done
              sleep 10
            fi
            
            # Delete ALB
            if aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN" 2>/dev/null; then
              echo "âœ… Existing ALB deleted to resolve conflict"
              echo "â³ Waiting for deletion to complete..."
              sleep 60
              
              # Also clean up orphaned Target Groups after ALB deletion
              echo "ğŸ§¹ Cleaning up orphaned Target Groups after ALB deletion..."
              TG_ARN=$(aws elbv2 describe-target-groups --names automax-dealership-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
              if [ "$TG_ARN" != "" ] && [ "$TG_ARN" != "None" ]; then
                echo "ğŸ”„ Found orphaned Target Group, deleting: $TG_ARN"
                if aws elbv2 delete-target-group --target-group-arn "$TG_ARN" 2>/dev/null; then
                  echo "âœ… Orphaned Target Group deleted"
                else
                  echo "âš ï¸ Failed to delete Target Group (might be in use)"
                fi
                sleep 10
              fi
            else
              echo "âŒ Failed to delete ALB. Manual intervention may be required."
            fi
          fi
        fi
        
        # 2. Handle Target Group (only if ALB wasn't deleted above)
        if [ "$LB_ARN" = "" ] || [ "$LB_ARN" = "None" ]; then
          echo "ğŸ” Checking for existing Target Group (since no ALB was found)..."
          TG_ARN=$(aws elbv2 describe-target-groups --names automax-dealership-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          
          if [ "$TG_ARN" != "" ] && [ "$TG_ARN" != "None" ]; then
            echo "ğŸ”„ Found orphaned Target Group, removing before import: $TG_ARN"
            # Delete orphaned target group
            if aws elbv2 delete-target-group --target-group-arn "$TG_ARN" 2>/dev/null; then
              echo "âœ… Orphaned Target Group deleted"
            else
              echo "âš ï¸ Failed to delete Target Group, will skip import"
            fi
            sleep 10
          fi
        else
          # Try to import Target Group if ALB still exists
          TG_ARN=$(aws elbv2 describe-target-groups --names automax-dealership-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          if ! import_if_exists "Target Group" "automax-dealership-tg" "$TG_ARN" "aws_lb_target_group.main"; then
            # If import fails, delete the conflicting Target Group
            if [ "$TG_ARN" != "" ] && [ "$TG_ARN" != "None" ]; then
              echo "âŒ Target Group import failed, deleting conflicting resource: $TG_ARN"
              aws elbv2 delete-target-group --target-group-arn "$TG_ARN" 2>/dev/null && echo "âœ… Conflicting Target Group deleted"
              sleep 10
            fi
          fi
        fi
        
        # 3. Handle CloudWatch Log Group
        LOG_GROUP_NAME="/ecs/automax-dealership"
        echo "ğŸ” Checking for existing CloudWatch Log Group..."
        if aws logs describe-log-groups --log-group-name-prefix "$LOG_GROUP_NAME" --query 'logGroups[0].logGroupName' --output text 2>/dev/null | grep -q "$LOG_GROUP_NAME"; then
          if ! import_if_exists "CloudWatch Log Group" "$LOG_GROUP_NAME" "$LOG_GROUP_NAME" "aws_cloudwatch_log_group.ecs"; then
            # If import fails, delete the existing log group
            echo "âŒ CloudWatch Log Group import failed, deleting existing log group..."
            if aws logs delete-log-group --log-group-name "$LOG_GROUP_NAME" 2>/dev/null; then
              echo "âœ… Existing CloudWatch Log Group deleted"
              sleep 5
            else
              echo "âš ï¸ Failed to delete CloudWatch Log Group"
            fi
          fi
        else
          echo "â„¹ï¸ No existing CloudWatch Log Group found"
        fi
        
        # 4. Handle IAM Roles
        echo "ğŸ” Checking for existing IAM Roles..."
        for role_name in "automax-dealership-ecs-task-execution-role" "automax-dealership-ecs-task-role"; do
          if aws iam get-role --role-name "$role_name" &>/dev/null; then
            echo "ğŸ”„ Found existing IAM Role: $role_name"
            
            if [ "$role_name" = "automax-dealership-ecs-task-execution-role" ]; then
              if ! import_if_exists "IAM Role" "$role_name" "$role_name" "aws_iam_role.ecs_task_execution"; then
                echo "âŒ IAM Role import failed for $role_name"
                echo "âš ï¸ Skipping IAM role deletion (requires careful handling)"
              fi
            else
              if ! import_if_exists "IAM Role" "$role_name" "$role_name" "aws_iam_role.ecs_task"; then
                echo "âŒ IAM Role import failed for $role_name"
                echo "âš ï¸ Skipping IAM role deletion (requires careful handling)"
              fi
            fi
          else
            echo "â„¹ï¸ IAM Role not found: $role_name"
          fi
        done
        
        echo "âœ… Pre-deployment state management completed"

    - name: ğŸ¯ ALB Conflict Resolution
      run: |
        echo "ğŸ¯ Running targeted ALB conflict resolution..."
        chmod +x resolve-alb-conflict.sh
        ./resolve-alb-conflict.sh
        echo "âœ… ALB conflict resolution completed"

    - name: ğŸ“‹ Terraform Plan
      run: |
        echo "ğŸ”„ Planning Terraform deployment..."
        
        # Create plan with detailed logging
        if ! terraform plan \
          -var="image_uri=${{ needs.build.outputs.image-tag }}" \
          -var="environment=production" \
          -detailed-exitcode \
          -out=tfplan; then
          
          PLAN_EXIT_CODE=$?
          echo "âš ï¸ Terraform plan exit code: $PLAN_EXIT_CODE"
          
          if [ $PLAN_EXIT_CODE -eq 2 ]; then
            echo "âœ… Plan succeeded with changes detected."
          else
            echo "âŒ Plan failed. Showing more details..."
            terraform plan \
              -var="image_uri=${{ needs.build.outputs.image-tag }}" \
              -var="environment=production" \
              -no-color
            exit 1
          fi
        else
          echo "âœ… Terraform plan completed successfully."
        fi

    - name: ğŸš€ Terraform Apply (with specific ALB conflict resolution)
      run: |
        echo "ğŸš€ Applying Terraform configuration..."
        
        # Capture apply output for error analysis
        if ! terraform apply -auto-approve tfplan 2>&1 | tee apply_output.log; then
          echo "âš ï¸ Initial apply failed. Analyzing error..."
          
          # Check if error is specifically about ALB already exists
          if grep -q "ELBv2 Load Balancer.*already exists" apply_output.log; then
            echo "ğŸ¯ Detected ALB 'already exists' error - this is the exact issue we're solving!"
            
            # Extract ALB name from error message
            ALB_NAME=$(grep "ELBv2 Load Balancer" apply_output.log | sed -n 's/.*ELBv2 Load Balancer (\([^)]*\)).*/\1/p' | head -n1)
            echo "ğŸ” Problematic ALB name: $ALB_NAME"
            
            # Get ALB ARN for deletion
            LB_ARN=$(aws elbv2 describe-load-balancers --names "$ALB_NAME" --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "")
            
            if [ "$LB_ARN" != "" ] && [ "$LB_ARN" != "None" ]; then
              echo "ğŸ”„ Found conflicting ALB: $LB_ARN"
              echo "ğŸ—‘ï¸ Deleting existing ALB to resolve 'already exists' conflict..."
              
              # Delete all listeners first
              LISTENERS=$(aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --query 'Listeners[].ListenerArn' --output text 2>/dev/null || echo "")
              if [ "$LISTENERS" != "" ]; then
                echo "ğŸ”„ Deleting listeners..."
                for LISTENER in $LISTENERS; do
                  aws elbv2 delete-listener --listener-arn "$LISTENER" 2>/dev/null && echo "âœ… Deleted listener: $LISTENER"
                done
                sleep 15
              fi
              
              # Now delete the ALB
              if aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN" 2>/dev/null; then
                echo "âœ… Successfully deleted conflicting ALB"
                echo "â³ Waiting for ALB deletion to propagate..."
                sleep 90  # Wait longer for full deletion
                
                # Verify deletion
                if ! aws elbv2 describe-load-balancers --names "$ALB_NAME" &>/dev/null; then
                  echo "âœ… ALB deletion confirmed"
                  
                  # Now retry the terraform apply
                  echo "ğŸ”„ Retrying Terraform apply after ALB conflict resolution..."
                  if terraform apply -auto-approve tfplan; then
                    echo "ğŸ‰ Terraform apply successful after ALB conflict resolution!"
                    exit 0
                  else
                    echo "âŒ Terraform apply still failing after ALB deletion"
                  fi
                else
                  echo "âš ï¸ ALB deletion not yet complete, continuing with full conflict resolution..."
                fi
              else
                echo "âŒ Failed to delete ALB, continuing with full conflict resolution..."
              fi
            else
              echo "âš ï¸ Could not find ALB to delete, continuing with full conflict resolution..."
            fi
          fi
          
          echo "ğŸ”„ Attempting comprehensive conflict resolution..."
          
          # Set continue on error for imports and cleanup
          set +e
          
          # First, handle ECS services that might reference old network configs
          echo "ğŸ”„ Checking for ECS service conflicts..."
          
          # Stop any existing ECS service that might have old network configuration
          if aws ecs describe-services --cluster automax-dealership-cluster --services automax-dealership-service --query 'services[0].serviceName' --output text 2>/dev/null | grep -q automax-dealership-service; then
            echo "ğŸ”„ Updating ECS service to desired count 0 to handle network changes..."
            aws ecs update-service --cluster automax-dealership-cluster --service automax-dealership-service --desired-count 0 2>/dev/null || echo "ECS service update skipped"
            sleep 30  # Wait for tasks to stop
          fi
          
          # First, try to release any unused EIPs to free up quota
          echo "ğŸ§¹ Cleaning up unused EIPs..."
          aws ec2 describe-addresses --query 'Addresses[?AssociationId==null].AllocationId' --output text | \
          xargs -n1 -I {} aws ec2 release-address --allocation-id {} 2>/dev/null || echo "No unused EIPs to release"
          
          # Handle existing load balancer and target group conflicts
          echo "ğŸ”„ Handling load balancer conflicts..."
          
          # Check if ALB exists in AWS (try multiple methods)
          echo "ğŸ” Checking for existing ALB..."
          LB_ARN=""
          
          # Method 1: Try by name
          LB_ARN=$(aws elbv2 describe-load-balancers --names automax-dealership-alb --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "")
          
          # Method 2: If name lookup fails, search by tags or list all
          if [ "$LB_ARN" = "" ] || [ "$LB_ARN" = "None" ]; then
            echo "ï¿½ ALB not found by name, searching all load balancers..."
            LB_ARN=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `automax-dealership`)].LoadBalancerArn' --output text 2>/dev/null | head -n1 || echo "")
          fi
          
          if [ "$LB_ARN" != "" ] && [ "$LB_ARN" != "None" ]; then
            echo "âœ… Found existing ALB: $LB_ARN"
            
            # Check if it's already in Terraform state
            if ! terraform state show aws_lb.main &>/dev/null; then
              echo "ğŸ”„ Importing ALB into Terraform state..."
              
              # Try importing by ARN first
              if terraform import 'aws_lb.main' "$LB_ARN"; then
                echo "âœ… ALB import by ARN successful"
              # If ARN import fails, try by name
              elif terraform import 'aws_lb.main' 'automax-dealership-alb'; then
                echo "âœ… ALB import by name successful"
              else
                echo "âŒ ALB import failed. Attempting to remove existing ALB to avoid conflicts..."
                
                # Get all listeners first
                LISTENERS=$(aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --query 'Listeners[].ListenerArn' --output text 2>/dev/null || echo "")
                
                # Delete listeners first
                if [ "$LISTENERS" != "" ]; then
                  echo "ğŸ”„ Deleting existing listeners..."
                  for LISTENER in $LISTENERS; do
                    aws elbv2 delete-listener --listener-arn "$LISTENER" 2>/dev/null || echo "Failed to delete listener $LISTENER"
                  done
                  sleep 10
                fi
                
                # Delete the load balancer
                echo "ğŸ”„ Deleting existing ALB to resolve conflict..."
                if aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN" 2>/dev/null; then
                  echo "âœ… ALB deleted successfully"
                  # Wait for ALB to be fully deleted
                  echo "â³ Waiting for ALB deletion to complete..."
                  sleep 60
                else
                  echo "âš ï¸ Failed to delete ALB. Pipeline will attempt to handle conflict."
                fi
              fi
            else
              echo "âœ… ALB already in Terraform state"
            fi
          else
            echo "â„¹ï¸ No existing ALB found"
          fi
          
          # Check if Target Group exists
          TG_ARN=$(aws elbv2 describe-target-groups --names automax-dealership-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          if [ "$TG_ARN" != "" ] && [ "$TG_ARN" != "None" ]; then
            echo "ğŸ”„ Target group exists in AWS: $TG_ARN"
            
            # Check if it's already in Terraform state
            if ! terraform state show aws_lb_target_group.main &>/dev/null; then
              echo "ğŸ”„ Importing Target Group into Terraform state..."
              if terraform import 'aws_lb_target_group.main' "$TG_ARN"; then
                echo "âœ… Target Group import successful"
              else
                echo "âš ï¸ Target Group import failed, will try to handle during apply"
              fi
            else
              echo "âœ… Target Group already in Terraform state"
            fi
            
            # Get listener ARN and import if exists
            if [ "$LB_ARN" != "" ] && [ "$LB_ARN" != "None" ]; then
              LISTENER_ARN=$(aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --query 'Listeners[0].ListenerArn' --output text 2>/dev/null || echo "")
              if [ "$LISTENER_ARN" != "" ] && [ "$LISTENER_ARN" != "None" ]; then
                echo "ğŸ”„ Listener exists in AWS: $LISTENER_ARN"
                
                # Check if it's already in Terraform state
                if ! terraform state show aws_lb_listener.main &>/dev/null; then
                  echo "ğŸ”„ Importing Listener into Terraform state..."
                  if terraform import 'aws_lb_listener.main' "$LISTENER_ARN"; then
                    echo "âœ… Listener import successful"
                  else
                    echo "âš ï¸ Listener import failed, will try to handle during apply"
                  fi
                else
                  echo "âœ… Listener already in Terraform state"
                fi
              fi
            fi
          fi
          
          # Import other resources that might exist
          echo "ğŸ”„ Importing other existing resources..."
          
          # CloudWatch Log Group
          if aws logs describe-log-groups --log-group-name-prefix "/ecs/automax-dealership" --query 'logGroups[0].logGroupName' --output text 2>/dev/null | grep -q "/ecs/automax-dealership"; then
            if ! terraform state show aws_cloudwatch_log_group.ecs &>/dev/null; then
              echo "ğŸ”„ Importing CloudWatch Log Group..."
              if terraform import 'aws_cloudwatch_log_group.ecs' '/ecs/automax-dealership' 2>/dev/null; then
                echo "âœ… CloudWatch Log Group import successful"
              else
                echo "âŒ CloudWatch Log Group import failed, deleting existing log group..."
                aws logs delete-log-group --log-group-name "/ecs/automax-dealership" 2>/dev/null || echo "Failed to delete log group"
                sleep 5
              fi
            fi
          fi
          
          # IAM Roles
          if aws iam get-role --role-name automax-dealership-ecs-task-execution-role &>/dev/null; then
            if ! terraform state show aws_iam_role.ecs_task_execution &>/dev/null; then
              echo "ğŸ”„ Importing ECS Task Execution Role..."
              terraform import 'aws_iam_role.ecs_task_execution' 'automax-dealership-ecs-task-execution-role' || echo "Task execution role import failed"
            fi
          fi
          
          if aws iam get-role --role-name automax-dealership-ecs-task-role &>/dev/null; then
            if ! terraform state show aws_iam_role.ecs_task &>/dev/null; then
              echo "ğŸ”„ Importing ECS Task Role..."
              terraform import 'aws_iam_role.ecs_task' 'automax-dealership-ecs-task-role' || echo "Task role import failed"
            fi
          fi
          
          # Re-enable exit on error
          set -e
          
          echo "ğŸ”„ Re-planning after imports and cleanup..."
          terraform plan \
            -var="image_uri=${{ needs.build.outputs.image-tag }}" \
            -var="environment=production" \
            -out=tfplan-retry
          
          echo "ğŸš€ Re-applying Terraform configuration..."
          terraform apply -auto-approve tfplan-retry
        fi
        
        echo "âœ… Terraform apply completed successfully!"
        
        # Verify that apply actually succeeded by checking critical outputs
        echo "ğŸ” Verifying Terraform apply success..."
        if terraform output ecs_cluster_name &>/dev/null; then
          echo "âœ… Terraform state contains expected outputs"
        else
          echo "âŒ Terraform apply may have failed - outputs not available"
          echo "ğŸ” Showing Terraform state:"
          terraform show -no-color || echo "No state available"
          exit 1
        fi

    - name: ğŸ“¤ Output Infrastructure Info
      run: |
        echo "ğŸ” Checking available Terraform outputs..."
        terraform output -json || echo "No outputs available yet"
        
        echo "ğŸ” Attempting to get infrastructure information..."
        if terraform output load_balancer_dns &>/dev/null; then
          echo "Load Balancer DNS: $(terraform output -raw load_balancer_dns)"
        else
          echo "âš ï¸ Load Balancer DNS output not available yet"
        fi
        
        if terraform output ecs_cluster_name &>/dev/null; then
          echo "ECS Cluster: $(terraform output -raw ecs_cluster_name)"
        else
          echo "âš ï¸ ECS Cluster name output not available yet"
        fi

    - name: âœ… Verify Infrastructure Readiness
      run: |
        echo "ğŸ” Verifying that all infrastructure components are ready..."
        
        # Check if outputs are available first
        if ! terraform output ecs_cluster_name &>/dev/null; then
          echo "âŒ Terraform outputs not available. Infrastructure may not be fully deployed."
          echo "ğŸ” Available outputs:"
          terraform output || echo "No outputs found"
          exit 1
        fi
        
        # Verify ECS Cluster
        CLUSTER_NAME=$(terraform output -raw ecs_cluster_name)
        echo "ğŸ” Verifying ECS cluster: $CLUSTER_NAME"
        if aws ecs describe-clusters --clusters "$CLUSTER_NAME" --query 'clusters[0].status' --output text | grep -q "ACTIVE"; then
          echo "âœ… ECS cluster is active"
        else
          echo "âŒ ECS cluster is not active"
          exit 1
        fi
        
        # Verify ECS Service
        if terraform output ecs_service_name &>/dev/null; then
          SERVICE_NAME=$(terraform output -raw ecs_service_name)
          echo "ğŸ” Verifying ECS service: $SERVICE_NAME"
          if aws ecs describe-services --cluster "$CLUSTER_NAME" --services "$SERVICE_NAME" --query 'services[0].serviceName' --output text 2>/dev/null | grep -q "$SERVICE_NAME"; then
            echo "âœ… ECS service exists"
          else
            echo "âŒ ECS service not found"
            exit 1
          fi
        else
          echo "âš ï¸ ECS service name output not available, skipping service verification"
        fi
        
        # Verify ALB (optional check since it might not be ready yet)
        if terraform output load_balancer_dns &>/dev/null; then
          ALB_DNS=$(terraform output -raw load_balancer_dns)
          echo "ğŸ” Verifying ALB: $ALB_DNS"
          if [ "$ALB_DNS" != "" ] && [ "$ALB_DNS" != "null" ]; then
            echo "âœ… ALB is ready"
          else
            echo "âš ï¸ ALB DNS not available"
          fi
        else
          echo "âš ï¸ ALB DNS output not available, skipping ALB verification"
        fi
        
        echo "ğŸ‰ Infrastructure verification completed (some components may still be initializing)"

  # Job 5: Deploy Application to ECS
  deploy-application:
    name: ğŸ¯ Deploy Application
    runs-on: ubuntu-latest
    needs: [build, deploy-infrastructure]
    if: github.ref == 'refs/heads/main'

    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: ğŸ“‹ Check and Download Task Definition
      run: |
        echo "ğŸ” Checking if task definition exists..."
        
        # Get the actual task definition name from the environment (now corrected)
        TASK_DEF_NAME="${{ env.ECS_TASK_DEFINITION }}"
        echo "ğŸ” Looking for task definition: $TASK_DEF_NAME"
        
        # First, try to get current Target Group ARN from AWS directly since Terraform isn't available in this job
        CURRENT_TG_ARN=""
        echo "ğŸ” Checking for current Target Group ARN from AWS..."
        CURRENT_TG_ARN=$(aws elbv2 describe-target-groups --names automax-dealership-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
        if [ "$CURRENT_TG_ARN" != "" ] && [ "$CURRENT_TG_ARN" != "None" ]; then
          echo "ğŸ” Found Target Group ARN from AWS: $CURRENT_TG_ARN"
        else
          echo "âŒ Could not find Target Group ARN in AWS. This indicates a deployment conflict."
          echo "ğŸ”„ Infrastructure exists but Target Group is missing - using force deployment"
          echo "force-deployment=true" >> $GITHUB_OUTPUT
          # Don't exit here - continue to check task definition for further conflicts
        fi
        
        # Check if task definition exists
        if aws ecs describe-task-definition --task-definition "$TASK_DEF_NAME" &>/dev/null; then
          echo "âœ… Task definition exists, downloading current version..."
          aws ecs describe-task-definition \
            --task-definition "$TASK_DEF_NAME" \
            --query taskDefinition > task-definition.json
          
          # Verify that the task definition doesn't reference a non-existent Target Group
          if grep -q "targetGroupArn" task-definition.json; then
            TASK_TG_ARN=$(jq -r '.loadBalancers[0].targetGroupArn' task-definition.json 2>/dev/null || echo "")
            if [ "$TASK_TG_ARN" != "" ] && [ "$TASK_TG_ARN" != "null" ]; then
              echo "ğŸ“‹ Task definition references Target Group ARN: $TASK_TG_ARN"
              
              # Check if this Target Group actually exists
              if ! aws elbv2 describe-target-groups --target-group-arns "$TASK_TG_ARN" &>/dev/null; then
                echo "âŒ Task definition references non-existent Target Group: $TASK_TG_ARN"
                echo "ğŸ”„ This is the source of the deployment error!"
                echo "ğŸ”„ Will use force deployment to bypass this conflict"
                echo "force-deployment=true" >> $GITHUB_OUTPUT
              elif [ "$CURRENT_TG_ARN" != "" ] && [ "$TASK_TG_ARN" != "$CURRENT_TG_ARN" ]; then
                echo "âš ï¸ Task definition references different Target Group ARN: $TASK_TG_ARN"
                echo "ğŸ”„ Current Target Group ARN: $CURRENT_TG_ARN"
                echo "ğŸ”„ Will use force deployment to update to current TG"
                echo "force-deployment=true" >> $GITHUB_OUTPUT
              else
                echo "âœ… Task definition Target Group ARN is valid and current"
              fi
            fi
          else
            echo "â„¹ï¸ Task definition has no Target Group ARN reference"
          fi
        else
          echo "âš ï¸ Task definition doesn't exist yet (first deployment)"
          echo "ğŸ”„ Terraform should have created it. Let's wait and retry..."
          
          # Wait a bit for Terraform resources to be fully ready
          sleep 30
          
          # Try again
          if aws ecs describe-task-definition --task-definition "$TASK_DEF_NAME" &>/dev/null; then
            echo "âœ… Task definition now exists, downloading..."
            aws ecs describe-task-definition \
              --task-definition "$TASK_DEF_NAME" \
              --query taskDefinition > task-definition.json
          else
            echo "âŒ Task definition still not found. This might be a first deployment."
            echo "ğŸ¯ Will trigger ECS service update instead of task definition update"
            echo "first-deployment=true" >> $GITHUB_OUTPUT
          fi
        fi
      id: check-task-def

    - name: ğŸ” Debug Deployment Strategy
      run: |
        echo "ğŸ” Deployment Strategy Decision:"
        echo "  first-deployment: ${{ steps.check-task-def.outputs.first-deployment }}"
        echo "  force-deployment: ${{ steps.check-task-def.outputs.force-deployment }}"
        
        if [ "${{ steps.check-task-def.outputs.force-deployment }}" == "true" ]; then
          echo "ğŸ”„ FORCE DEPLOYMENT: Bypassing aws-actions due to Target Group conflicts"
        elif [ "${{ steps.check-task-def.outputs.first-deployment }}" == "true" ]; then
          echo "ğŸ¯ FIRST DEPLOYMENT: Using direct ECS service update"
        else
          echo "âœ… NORMAL DEPLOYMENT: Using aws-actions/amazon-ecs-deploy-task-definition"
        fi

    - name: ğŸ”„ Update Task Definition (if exists)
      id: task-def
      if: steps.check-task-def.outputs.first-deployment != 'true' && steps.check-task-def.outputs.force-deployment != 'true'
      uses: aws-actions/amazon-ecs-render-task-definition@v1
      with:
        task-definition: task-definition.json
        container-name: automax-container
        image: ${{ needs.build.outputs.image-tag }}

    - name: ğŸš€ Deploy to Amazon ECS (Update Existing)
      if: steps.check-task-def.outputs.first-deployment != 'true' && steps.check-task-def.outputs.force-deployment != 'true'
      uses: aws-actions/amazon-ecs-deploy-task-definition@v1
      with:
        task-definition: ${{ steps.task-def.outputs.task-definition }}
        service: ${{ env.ECS_SERVICE }}
        cluster: ${{ env.ECS_CLUSTER }}
        wait-for-service-stability: true

    - name: ğŸ¯ Deploy to Amazon ECS (First Deployment or Force Update)
      if: steps.check-task-def.outputs.first-deployment == 'true' || steps.check-task-def.outputs.force-deployment == 'true'
      run: |
        if [ "${{ steps.check-task-def.outputs.force-deployment }}" == "true" ]; then
          echo "ğŸ”„ Force deployment triggered due to Target Group ARN mismatch"
          echo "This avoids the 'target group does not exist' error"
        else
          echo "ğŸ¯ First deployment detected - verifying ECS infrastructure..."
        fi
        
        # Use environment variables instead of Terraform outputs
        # These should match what Terraform creates
        CLUSTER_NAME="${{ env.ECS_CLUSTER }}"
        SERVICE_NAME="${{ env.ECS_SERVICE }}"
        
        echo "ğŸ” Using environment-defined resources:"
        echo "  Cluster: $CLUSTER_NAME"
        echo "  Service: $SERVICE_NAME"
        
        # First, verify that the ECS cluster exists
        echo "ğŸ” Checking if ECS cluster exists..."
        if ! aws ecs describe-clusters --clusters "$CLUSTER_NAME" --query 'clusters[0].clusterName' --output text 2>/dev/null | grep -q "$CLUSTER_NAME"; then
          echo "âŒ ECS cluster not found: $CLUSTER_NAME"
          echo "ğŸ”„ This might be a Terraform timing issue. Waiting for cluster creation..."
          sleep 30
          
          # Check again
          if ! aws ecs describe-clusters --clusters "$CLUSTER_NAME" --query 'clusters[0].clusterName' --output text 2>/dev/null | grep -q "$CLUSTER_NAME"; then
            echo "âŒ ECS cluster still not found after waiting. Terraform may have failed to create it."
            echo "ğŸ” Listing all available clusters:"
            aws ecs list-clusters --query 'clusterArns' --output table || echo "No clusters found"
            exit 1
          fi
        fi
        echo "âœ… ECS cluster found: $CLUSTER_NAME"
        
        # Next, verify that the ECS service exists
        echo "ğŸ” Checking if ECS service exists..."
        if ! aws ecs describe-services --cluster "$CLUSTER_NAME" --services "$SERVICE_NAME" --query 'services[0].serviceName' --output text 2>/dev/null | grep -q "$SERVICE_NAME"; then
          echo "âŒ ECS service not found: $SERVICE_NAME"
          echo "ğŸ”„ This might be a Terraform timing issue. Waiting for service creation..."
          sleep 30
          
          # Check again
          if ! aws ecs describe-services --cluster "$CLUSTER_NAME" --services "$SERVICE_NAME" --query 'services[0].serviceName' --output text 2>/dev/null | grep -q "$SERVICE_NAME"; then
            echo "âŒ ECS service still not found after waiting. Terraform may have failed to create it."
            echo "ğŸ” Listing all services in cluster:"
            aws ecs list-services --cluster "$CLUSTER_NAME" --query 'serviceArns' --output table || echo "No services found"
            exit 1
          fi
        fi
        echo "âœ… ECS service found: $SERVICE_NAME"
        
        # Now safely trigger the service update with new image
        echo "ğŸš€ Updating ECS service with new image..."
        echo "ğŸ” New image: ${{ needs.build.outputs.image-tag }}"
        
        # Update the service to use the new image by forcing a new deployment
        # This bypasses the Target Group ARN issue by using the service's current configuration
        aws ecs update-service \
          --cluster "$CLUSTER_NAME" \
          --service "$SERVICE_NAME" \
          --force-new-deployment
        
        echo "âœ… ECS service deployment triggered"
        
        # Wait for deployment to complete
        echo "â³ Waiting for service to stabilize..."
        aws ecs wait services-stable \
          --cluster "$CLUSTER_NAME" \
          --services "$SERVICE_NAME"
        
        echo "âœ… ECS service deployment completed"

    - name: âœ… Verify Deployment
      run: |
        # Wait for deployment to complete
        sleep 30
        
        # Try to get load balancer DNS from AWS directly since Terraform isn't available in this job
        echo "ğŸ” Looking for Load Balancer..."
        LB_DNS=$(aws elbv2 describe-load-balancers --names automax-dealership-alb --query 'LoadBalancers[0].DNSName' --output text 2>/dev/null || echo "")
        
        if [ "$LB_DNS" != "" ] && [ "$LB_DNS" != "None" ]; then
          echo "ğŸ” Found Load Balancer DNS: $LB_DNS"
          
          # Check target group health first
          echo "ğŸ” Checking Target Group health..."
          TG_ARN=$(aws elbv2 describe-target-groups --names automax-dealership-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          
          if [ "$TG_ARN" != "" ] && [ "$TG_ARN" != "None" ]; then
            HEALTHY_TARGETS=$(aws elbv2 describe-target-health --target-group-arn "$TG_ARN" --query 'TargetHealthDescriptions[?TargetHealth.State==`healthy`] | length(@)' --output text 2>/dev/null || echo "0")
            TOTAL_TARGETS=$(aws elbv2 describe-target-health --target-group-arn "$TG_ARN" --query 'length(TargetHealthDescriptions)' --output text 2>/dev/null || echo "0")
            
            echo "ğŸ“Š Target Health: $HEALTHY_TARGETS/$TOTAL_TARGETS healthy"
            
            if [ "$TOTAL_TARGETS" -eq 0 ]; then
              echo "âŒ No targets registered - ECS tasks not connecting to load balancer"
              echo "ğŸ’¡ This is likely why the load balancer shows no content"
              echo "ğŸ”§ Run './quick-fix.sh' to attempt automatic resolution"
            elif [ "$HEALTHY_TARGETS" -eq 0 ]; then
              echo "âŒ No healthy targets - ECS tasks failing health checks"
              echo "ğŸ’¡ Check ECS task logs for container startup issues"
              echo "ğŸ”§ Run './debug-deployment.sh' for detailed diagnostics"
            else
              echo "âœ… Healthy targets found - load balancer should be working"
            fi
          else
            echo "âŒ Could not check target group health"
          fi
          
          # Health check (try both root and /health endpoints)
          echo "ğŸ” Checking application health at: http://$LB_DNS/"
          ROOT_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "http://$LB_DNS/" --connect-timeout 30 --max-time 60 || echo "000")
          HEALTH_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "http://$LB_DNS/health" --connect-timeout 30 --max-time 60 || echo "000")
          
          echo "ğŸ“Š HTTP Status Codes:"
          echo "  Root path (/): $ROOT_STATUS"
          echo "  Health path (/health): $HEALTH_STATUS"
          
          if [ "$ROOT_STATUS" == "200" ]; then
            echo "âœ… Application is responding successfully on root path!"
            echo "ğŸ‰ Website is live and ready!"
          elif [ "$HEALTH_STATUS" == "200" ]; then
            echo "âœ… Application is responding on health endpoint"
            echo "âš ï¸ Root path might need more time to initialize"
          else
            echo "âš ï¸ Application not yet responding (Status: $ROOT_STATUS)"
            echo "ğŸ” This might be normal for a fresh deployment (ECS tasks starting up)"
            echo "â° Wait 2-3 minutes and try again"
            echo ""
            echo "ğŸ”§ TROUBLESHOOTING:"
            echo "  1. Run './quick-fix.sh' to force restart ECS tasks"
            echo "  2. Run './debug-deployment.sh' for detailed diagnostics"
            echo "  3. Check ECS logs: aws logs tail '/ecs/automax-dealership' --follow"
          fi
          
          echo ""
          echo "ğŸ‰ Deployment completed!"
          echo "ğŸŒ Application URL: http://$LB_DNS"
          echo "ğŸ“‹ If the site shows no content, run the troubleshooting scripts provided"
        else
          echo "âš ï¸ Load Balancer DNS not available from AWS"
          echo "ğŸ” Deployment completed, but unable to verify application URL"
          echo "âœ… ECS service should be running - check AWS console for details"
        fi

  # Job 6: Post-Deployment Notifications
  notify:
    name: ğŸ“¢ Notify Deployment Status
    runs-on: ubuntu-latest
    needs: [deploy-application]
    if: always()

    steps:
    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
    - name: ğŸ“¢ Deployment Notification - Success
      if: ${{ needs.deploy-application.result == 'success' }}
      run: |
        echo "ğŸ‰ SUCCESS: AutoMax Car Dealership deployed successfully!"
        echo "âœ… All deployment steps completed successfully"
        echo ""
        echo "ğŸŒ ACCESS YOUR WEBSITE:"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        
        # Get the Load Balancer DNS name
        LB_DNS=$(aws elbv2 describe-load-balancers --names automax-dealership-alb --query 'LoadBalancers[0].DNSName' --output text 2>/dev/null || echo "")
        
        if [ "$LB_DNS" != "" ] && [ "$LB_DNS" != "None" ]; then
          echo "ğŸ”— Website URL: http://$LB_DNS"
          echo "ğŸ“± Mobile-friendly: http://$LB_DNS"
          echo "ğŸ” You can bookmark this URL - it won't change unless you rebuild the infrastructure"
          echo ""
          echo "â° Note: It may take 2-3 minutes for the application to be fully ready"
          echo "ğŸ”„ If you get 503 errors initially, wait a moment and refresh"
        else
          echo "âš ï¸  Could not retrieve Load Balancer DNS automatically"
          echo "ğŸ” To find your website URL:"
          echo "   1. Go to AWS Console â†’ EC2 â†’ Load Balancers"
          echo "   2. Find 'automax-dealership-alb'"
          echo "   3. Copy the DNS name and access via http://[DNS-NAME]"
        fi
        
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo ""
        echo "ğŸ“Š DEPLOYMENT SUMMARY:"
        echo "  â€¢ Docker Image: Built and pushed to ECR"
        echo "  â€¢ Infrastructure: ECS Cluster, ALB, Target Groups created"
        echo "  â€¢ Application: Deployed to ECS Fargate"
        echo "  â€¢ Security: Security groups configured for web access"
        echo "  â€¢ Status: Ready for traffic âœ…"

    - name: ğŸ“‹ Create Deployment Summary
      if: ${{ needs.deploy-application.result == 'success' }}
      run: |
        echo "## ğŸš— AutoMax Car Dealership - Deployment Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### âœ… Deployment Status: SUCCESS" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Get the Load Balancer DNS name for the summary
        LB_DNS=$(aws elbv2 describe-load-balancers --names automax-dealership-alb --query 'LoadBalancers[0].DNSName' --output text 2>/dev/null || echo "")
        
        if [ "$LB_DNS" != "" ] && [ "$LB_DNS" != "None" ]; then
          echo "### ğŸŒ Website Access" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**ğŸ”— Your website is now live at:**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**[http://$LB_DNS](http://$LB_DNS)**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "> ğŸ’¡ **Bookmark this URL** - it will remain the same for future deployments" >> $GITHUB_STEP_SUMMARY
          echo "> â° Allow 2-3 minutes for the application to be fully ready after deployment" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        else
          echo "### ğŸŒ Website Access" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "To access your website:" >> $GITHUB_STEP_SUMMARY
          echo "1. Go to [AWS Console â†’ EC2 â†’ Load Balancers](https://console.aws.amazon.com/ec2/v2/home#LoadBalancers:)" >> $GITHUB_STEP_SUMMARY
          echo "2. Find **automax-dealership-alb**" >> $GITHUB_STEP_SUMMARY
          echo "3. Copy the DNS name and access via http://[DNS-NAME]" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "### ğŸ“Š Infrastructure Components" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Component | Status | Details |" >> $GITHUB_STEP_SUMMARY
        echo "|-----------|---------|---------|" >> $GITHUB_STEP_SUMMARY
        echo "| ğŸ³ Docker Image | âœ… Built & Pushed | ECR Repository: automax-dealership |" >> $GITHUB_STEP_SUMMARY
        echo "| â˜ï¸ ECS Cluster | âœ… Running | Cluster: automax-dealership-cluster |" >> $GITHUB_STEP_SUMMARY
        echo "| ğŸš€ ECS Service | âœ… Active | Service: automax-dealership-service |" >> $GITHUB_STEP_SUMMARY
        echo "| âš–ï¸ Load Balancer | âœ… Available | ALB: automax-dealership-alb |" >> $GITHUB_STEP_SUMMARY
        echo "| ğŸ”’ Security | âœ… Configured | Security groups for web traffic |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ’° Cost Information" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Optimized for AWS Free Tier**" >> $GITHUB_STEP_SUMMARY
        echo "- Single ECS task (0.25 vCPU, 0.5 GB memory)" >> $GITHUB_STEP_SUMMARY
        echo "- Application Load Balancer (first 15 LCU-hours free monthly)" >> $GITHUB_STEP_SUMMARY
        echo "- Data transfer: First 1 GB free monthly" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "---" >> $GITHUB_STEP_SUMMARY
        echo "*Deployment completed at: $(date)*" >> $GITHUB_STEP_SUMMARY

    - name: ğŸ“¢ Deployment Notification - Failure
      if: ${{ needs.deploy-application.result == 'failure' }}
      run: |
        echo "âŒ FAILURE: AutoMax Car Dealership deployment failed!"
        echo "ğŸ” Check the logs above for detailed error information"
        echo "ğŸ’¡ Common issues: Target Group conflicts, infrastructure timing, or resource limits"

    - name: ğŸ“¢ Slack Notification - Success (Optional)
      if: ${{ needs.deploy-application.result == 'success' }}
      uses: 8398a7/action-slack@v3
      with:
        status: success
        text: 'ğŸš— AutoMax Car Dealership deployed successfully!'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      continue-on-error: true

    - name: ğŸ“¢ Slack Notification - Failure (Optional)
      if: ${{ needs.deploy-application.result == 'failure' }}
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        text: 'âŒ AutoMax Car Dealership deployment failed!'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      continue-on-error: true
