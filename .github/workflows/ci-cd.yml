name: CI/CD Pipeline - AutoMax Car Dealership

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: automax-dealership
  ECS_SERVICE: automax-service
  ECS_CLUSTER: automax-cluster
  ECS_TASK_DEFINITION: automax-task-definition

jobs:
  # Job 1: Code Quality and Testing
  test:
    name: ğŸ§ª Test & Quality Checks
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: ğŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest requests

    - name: ğŸ” Lint HTML Files
      run: |
        # Install HTML linter
        npm install -g htmlhint
        htmlhint index.html

    - name: ğŸ¨ Lint CSS Files
      run: |
        # Install CSS linter
        npm install -g stylelint stylelint-config-standard
        echo '{"extends": "stylelint-config-standard"}' > .stylelintrc.json
        stylelint "*.css" || true  # Allow to continue even with warnings

    - name: âš¡ Lint JavaScript Files
      run: |
        # Install JS linter
        npm install -g eslint
        npx eslint --init --yes || true
        npx eslint "*.js" || true  # Allow to continue even with warnings

    - name: ğŸ§ª Run Website Tests
      run: |
        # Start a simple HTTP server for testing
        python -m http.server 8000 &
        sleep 5
        
        # Run Python tests
        export TEST_URL="http://localhost:8000"
        python -m pytest tests/ -v --tb=short

    - name: ğŸ“Š Generate Test Report
      if: always()
      run: |
        echo "Test execution completed"
        echo "Timestamp: $(date)"

  # Job 2: Create ECR Repository
  create-ecr:
    name: ğŸ—ï¸ Create ECR Repository
    runs-on: ubuntu-latest
    needs: test
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: ğŸ—ï¸ Create ECR Repository if not exists
      run: |
        # Check if repository exists, create if it doesn't
        aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} || \
        aws ecr create-repository \
          --repository-name ${{ env.ECR_REPOSITORY }} \
          --region ${{ env.AWS_REGION }} \
          --image-scanning-configuration scanOnPush=true \
          --encryption-configuration encryptionType=AES256
        
        echo "âœ… ECR repository ready: ${{ env.ECR_REPOSITORY }}"

  # Job 3: Build and Push Docker Image
  build:
    name: ğŸ³ Build & Push Docker Image
    runs-on: ubuntu-latest
    needs: [test, create-ecr]
    if: github.ref == 'refs/heads/main'
    
    outputs:
      image-tag: ${{ steps.image-uri.outputs.uri }}
      image-digest: ${{ steps.build.outputs.digest }}

    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: ğŸ”‘ Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2

    - name: ğŸ·ï¸ Extract Metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: ğŸ› ï¸ Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: ğŸ³ Build and Push Docker Image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: ğŸ·ï¸ Set Image URI Output
      id: image-uri
      run: |
        # Create a clean, single image URI for Terraform
        IMAGE_URI="${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest"
        echo "uri=${IMAGE_URI}" >> $GITHUB_OUTPUT
        echo "âœ… Image URI: ${IMAGE_URI}"

    - name: ğŸ” Scan Image for Vulnerabilities
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest
        format: 'sarif'
        output: 'trivy-results.sarif'
        exit-code: '0'  # Don't fail the build on vulnerabilities
        severity: 'CRITICAL,HIGH,MEDIUM'
      continue-on-error: true

    - name: ğŸ“‹ Upload Trivy Scan Results to GitHub Security
      uses: github/codeql-action/upload-sarif@v3
      if: always() && hashFiles('trivy-results.sarif') != ''
      with:
        sarif_file: 'trivy-results.sarif'
        category: 'trivy-container-scan'
      continue-on-error: true

    - name: ğŸ” Alternative Security Scan (Local Image)
      if: always()
      run: |
        echo "ğŸ”’ Running security scan on locally built image..."
        
        # Use the local image that was just built (no ECR pull needed)
        LOCAL_IMAGE="${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest"
        
        # Check if the local image exists
        if docker image inspect "$LOCAL_IMAGE" >/dev/null 2>&1; then
          echo "âœ… Scanning local image: $LOCAL_IMAGE"
          # Run Trivy scan on local image
          docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
            aquasec/trivy:latest image \
            --format table \
            --severity HIGH,CRITICAL \
            "$LOCAL_IMAGE" || true
        else
          echo "âš ï¸ Local image not found, scanning base nginx:alpine instead"
          # Fallback to scanning base image
          docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
            aquasec/trivy:latest image \
            --format table \
            --severity HIGH,CRITICAL \
            nginx:alpine || true
        fi
        
        echo "âœ… Security scan completed"

    - name: ğŸ“Š Generate Security Report Summary
      if: always()
      run: |
        echo "## ğŸ”’ Security Scan Summary" >> $GITHUB_STEP_SUMMARY
        echo "- Image: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest" >> $GITHUB_STEP_SUMMARY
        echo "- Scan Date: $(date)" >> $GITHUB_STEP_SUMMARY
        if [ -f "trivy-results.sarif" ]; then
          echo "- âœ… SARIF results generated for GitHub Security tab" >> $GITHUB_STEP_SUMMARY
        else
          echo "- âš ï¸ SARIF upload skipped (check repository permissions)" >> $GITHUB_STEP_SUMMARY
        fi
        echo "- ğŸ“Š Table format scan completed (see logs above)" >> $GITHUB_STEP_SUMMARY

  # Job 4: Deploy Infrastructure with Terraform
  deploy-infrastructure:
    name: ğŸš€ Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: [test, build]
    if: github.ref == 'refs/heads/main'
    
    defaults:
      run:
        working-directory: terraform

    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: âš™ï¸ Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.6.0

    - name: ğŸ”§ Terraform Init
      run: terraform init

    - name: ğŸ§¹ Clean Up Existing Resources (if needed)
      run: |
        echo "ğŸ§¹ Checking for existing resources that might conflict..."
        
        # Check AWS account limits and existing resources
        echo "ğŸ“Š Checking AWS account limits..."
        
        # Check EIP usage
        EIP_LIMIT=$(aws ec2 describe-account-attributes --attribute-names max-elastic-ips --query 'AccountAttributes[0].AttributeValues[0].AttributeValue' --output text)
        EIP_USED=$(aws ec2 describe-addresses --query 'length(Addresses)' --output text)
        echo "EIP Limit: $EIP_LIMIT, Used: $EIP_USED"
        
        if [ "$EIP_USED" -ge "$EIP_LIMIT" ]; then
          echo "âš ï¸ EIP limit reached. Will use public subnet architecture without NAT gateways."
        fi
        
        # List existing resources for import
        if terraform state list | grep -E "(aws_lb\.main|aws_lb_target_group\.main|aws_cloudwatch_log_group\.ecs|aws_iam_role\.ecs_task_execution|aws_iam_role\.ecs_task)"; then
          echo "âš ï¸ Found existing resources in state. Will handle conflicts during apply."
        else
          echo "âœ… No conflicting resources found in Terraform state."
        fi

    - name: ğŸ“‹ Terraform Plan
      run: |
        echo "ğŸ”„ Planning Terraform deployment..."
        
        # Create plan with detailed logging
        if ! terraform plan \
          -var="image_uri=${{ needs.build.outputs.image-tag }}" \
          -var="environment=production" \
          -detailed-exitcode \
          -out=tfplan; then
          
          PLAN_EXIT_CODE=$?
          echo "âš ï¸ Terraform plan exit code: $PLAN_EXIT_CODE"
          
          if [ $PLAN_EXIT_CODE -eq 2 ]; then
            echo "âœ… Plan succeeded with changes detected."
          else
            echo "âŒ Plan failed. Showing more details..."
            terraform plan \
              -var="image_uri=${{ needs.build.outputs.image-tag }}" \
              -var="environment=production" \
              -no-color
            exit 1
          fi
        else
          echo "âœ… Terraform plan completed successfully."
        fi

    - name: ğŸš€ Terraform Apply (with conflict resolution)
      run: |
        echo "ğŸš€ Applying Terraform configuration..."
        
        # Try normal apply first
        if ! terraform apply -auto-approve tfplan; then
          echo "âš ï¸ Initial apply failed, likely due to existing resources or account limits."
          echo "ğŸ”„ Attempting to resolve conflicts..."
          
          # Set continue on error for imports and cleanup
          set +e
          
          # First, handle ECS services that might reference old network configs
          echo "ğŸ”„ Checking for ECS service conflicts..."
          
          # Stop any existing ECS service that might have old network configuration
          if aws ecs describe-services --cluster automax-dealership-cluster --services automax-dealership-service --query 'services[0].serviceName' --output text 2>/dev/null | grep -q automax-dealership-service; then
            echo "ğŸ”„ Updating ECS service to desired count 0 to handle network changes..."
            aws ecs update-service --cluster automax-dealership-cluster --service automax-dealership-service --desired-count 0 2>/dev/null || echo "ECS service update skipped"
            sleep 30  # Wait for tasks to stop
          fi
          
          # First, try to release any unused EIPs to free up quota
          echo "ğŸ§¹ Cleaning up unused EIPs..."
          aws ec2 describe-addresses --query 'Addresses[?AssociationId==null].AllocationId' --output text | \
          xargs -n1 -I {} aws ec2 release-address --allocation-id {} 2>/dev/null || echo "No unused EIPs to release"
          
          # Handle existing load balancer and target group conflicts
          echo "ğŸ”„ Handling load balancer conflicts..."
          
          # If target group exists and is attached to listener, we need to import both
          TG_ARN=$(aws elbv2 describe-target-groups --names automax-dealership-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          if [ "$TG_ARN" != "" ] && [ "$TG_ARN" != "None" ]; then
            echo "ğŸ”„ Target group exists: $TG_ARN"
            
            # Get the load balancer ARN
            LB_ARN=$(aws elbv2 describe-load-balancers --names automax-dealership-alb --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "")
            if [ "$LB_ARN" != "" ] && [ "$LB_ARN" != "None" ]; then
              echo "ğŸ”„ Load balancer exists: $LB_ARN"
              
              # Get listener ARN
              LISTENER_ARN=$(aws elbv2 describe-listeners --load-balancer-arn "$LB_ARN" --query 'Listeners[0].ListenerArn' --output text 2>/dev/null || echo "")
              
              # Import in correct order
              terraform import 'aws_lb.main' 'automax-dealership-alb' 2>/dev/null || echo "ALB import skipped"
              terraform import 'aws_lb_target_group.main' "$TG_ARN" 2>/dev/null || echo "TG import skipped"
              
              if [ "$LISTENER_ARN" != "" ] && [ "$LISTENER_ARN" != "None" ]; then
                terraform import 'aws_lb_listener.main' "$LISTENER_ARN" 2>/dev/null || echo "Listener import skipped"
              fi
            fi
          fi
          
          # Import other resources
          terraform import 'aws_cloudwatch_log_group.ecs' '/ecs/automax-dealership' 2>/dev/null || echo "Log group import skipped"
          terraform import 'aws_iam_role.ecs_task_execution' 'automax-dealership-ecs-task-execution-role' 2>/dev/null || echo "Task execution role import skipped"
          terraform import 'aws_iam_role.ecs_task' 'automax-dealership-ecs-task-role' 2>/dev/null || echo "Task role import skipped"
          
          # Re-enable exit on error
          set -e
          
          echo "ğŸ”„ Re-planning after imports and cleanup..."
          terraform plan \
            -var="image_uri=${{ needs.build.outputs.image-tag }}" \
            -var="environment=production" \
            -out=tfplan-retry
          
          echo "ğŸš€ Re-applying Terraform configuration..."
          terraform apply -auto-approve tfplan-retry
        fi
        
        echo "âœ… Terraform apply completed successfully!"

    - name: ğŸ“¤ Output Infrastructure Info
      run: |
        echo "Load Balancer DNS: $(terraform output -raw load_balancer_dns)"
        echo "ECS Cluster: $(terraform output -raw ecs_cluster_name)"

  # Job 5: Deploy Application to ECS
  deploy-application:
    name: ğŸ¯ Deploy Application
    runs-on: ubuntu-latest
    needs: [build, deploy-infrastructure]
    if: github.ref == 'refs/heads/main'

    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: ğŸ“‹ Check and Download Task Definition
      run: |
        echo "ğŸ” Checking if task definition exists..."
        
        # Check if task definition exists
        if aws ecs describe-task-definition --task-definition ${{ env.ECS_TASK_DEFINITION }} &>/dev/null; then
          echo "âœ… Task definition exists, downloading current version..."
          aws ecs describe-task-definition \
            --task-definition ${{ env.ECS_TASK_DEFINITION }} \
            --query taskDefinition > task-definition.json
        else
          echo "âš ï¸ Task definition doesn't exist yet (first deployment)"
          echo "ğŸ”„ Terraform should have created it. Let's wait and retry..."
          
          # Wait a bit for Terraform resources to be fully ready
          sleep 30
          
          # Try again
          if aws ecs describe-task-definition --task-definition ${{ env.ECS_TASK_DEFINITION }} &>/dev/null; then
            echo "âœ… Task definition now exists, downloading..."
            aws ecs describe-task-definition \
              --task-definition ${{ env.ECS_TASK_DEFINITION }} \
              --query taskDefinition > task-definition.json
          else
            echo "âŒ Task definition still not found. This might be a first deployment."
            echo "ğŸ¯ Will trigger ECS service update instead of task definition update"
            echo "first-deployment=true" >> $GITHUB_OUTPUT
          fi
        fi
      id: check-task-def

    - name: ğŸ”„ Update Task Definition (if exists)
      id: task-def
      if: steps.check-task-def.outputs.first-deployment != 'true'
      uses: aws-actions/amazon-ecs-render-task-definition@v1
      with:
        task-definition: task-definition.json
        container-name: automax-container
        image: ${{ needs.build.outputs.image-tag }}

    - name: ğŸš€ Deploy to Amazon ECS (Update Existing)
      if: steps.check-task-def.outputs.first-deployment != 'true'
      uses: aws-actions/amazon-ecs-deploy-task-definition@v1
      with:
        task-definition: ${{ steps.task-def.outputs.task-definition }}
        service: ${{ env.ECS_SERVICE }}
        cluster: ${{ env.ECS_CLUSTER }}
        wait-for-service-stability: true

    - name: ğŸ¯ Deploy to Amazon ECS (First Deployment)
      if: steps.check-task-def.outputs.first-deployment == 'true'
      run: |
        echo "ğŸ¯ First deployment detected - triggering ECS service update..."
        
        # Force new deployment to pick up the Terraform-created task definition
        aws ecs update-service \
          --cluster ${{ env.ECS_CLUSTER }} \
          --service ${{ env.ECS_SERVICE }} \
          --force-new-deployment
        
        echo "âœ… ECS service deployment triggered"
        
        # Wait for deployment to complete
        echo "â³ Waiting for service to stabilize..."
        aws ecs wait services-stable \
          --cluster ${{ env.ECS_CLUSTER }} \
          --services ${{ env.ECS_SERVICE }}
        
        echo "âœ… ECS service deployment completed"

    - name: âœ… Verify Deployment
      run: |
        # Wait for deployment to complete
        sleep 30
        
        # Get load balancer URL from Terraform output
        LB_URL=$(cd terraform && terraform output -raw load_balancer_dns)
        
        # Health check
        echo "Checking application health at: http://$LB_URL/health"
        curl -f "http://$LB_URL/health" || exit 1
        
        echo "ğŸ‰ Deployment successful!"
        echo "Application is available at: http://$LB_URL"

  # Job 6: Post-Deployment Notifications
  notify:
    name: ğŸ“¢ Notify Deployment Status
    runs-on: ubuntu-latest
    needs: [deploy-application]
    if: always()

    steps:
    - name: ğŸ“¢ Slack Notification - Success
      if: ${{ needs.deploy-application.result == 'success' }}
      uses: 8398a7/action-slack@v3
      with:
        status: success
        text: 'ğŸš— AutoMax Car Dealership deployed successfully!'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}

    - name: ğŸ“¢ Slack Notification - Failure
      if: ${{ needs.deploy-application.result == 'failure' }}
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        text: 'âŒ AutoMax Car Dealership deployment failed!'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
